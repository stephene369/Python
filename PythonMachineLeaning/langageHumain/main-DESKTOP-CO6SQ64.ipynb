{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction au traitement de langage humain\n",
    "### 1\n",
    "- On souhaite creer un parefeux capable de bloquer les mails ayant un contenu particuler ; cette procedure fait appel au traitement du langage naturel ou Naturel Langage Processing (NLP)\n",
    "- Le NLP est la capacite pour un programme informatique de comprendre le langage humain telle qu'il est parlé ou écrit, il fait partit des technologie d'IA."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Exemple de programme utilisant le NLP \n",
    "- Le robot conversationel (ChatBot) ;\n",
    "- L'analyse des mails pour la detection de spam ;\n",
    "- La détection de virus ;\n",
    "- La traduction automatique ;\n",
    "-\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Etape du NLP\n",
    "Pour realiser un programme de NLP, nous avons besoin d'un echantillon de texte \n",
    "cet echantillon peut etre un echantillon de mail recu, des SMS, le contenu d'un livre etc \n",
    "Ensuite nous suivons les étapes ci-apres\n",
    "- a : La tokenisation \n",
    "    - Elle consiste a convertir des caratere qui compose un texte en une liste de jeton. Lors de ce processus,\n",
    "    on supprime :\n",
    "        - Les ponctuations ;\n",
    "        - Les mots fréquements utilisées (article , pronom , etc .. ) ; \n",
    "A la fin de cette étape on obtient la liste de token ou jeton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Application de  la tokenisation \n",
    "texte = \"Je suis allé le voir, lui et moi avons quitté Calavi pour Dassa\"\n",
    "## suppresion des ponctuations\n",
    "import string \n",
    "texteOutPonctuation = [char for char in texte if char not in string.punctuation]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Elimination des stops words du test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for text_ in texteOutPonctuation : text+=text_\n",
    "clean_text = [word for word in text.split() if word.lower() not in stopwords.words('french')]\n",
    "clean_text = ''.join(clean_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La vectorisation  \n",
    "Le principale probleme avec les jetons obtenu est qu'il sont des chaines de caracteres. Il nous faut les transformer en valeur numerque utilisable par les algorithmes.\n",
    "Cela ce fais en trois etape en utilisant l'aproche bag-of-words\n",
    "\n",
    "- ETAPE 1\n",
    "Il s'agit de compter le nombre de fois qu'un mot apparait dans un test\n",
    "Tf  = frquence du mot\n",
    "Exemple ;\n",
    "- Considérons un document contenant 100mots, dans lequel le mot chat =3 ;\n",
    "chatFreq = 3/100\n",
    "\n",
    "- ETAPE 2  INVERSE DOCUMENT FREQUENCY\n",
    "    - il s'agit de ponderer les decomptes effecter precedement, de sorte que les jetons frquent obtiennent un poid inferieur\n",
    "    - IDF = log((nombre total de texte)/(nombre total de text contenant le terme t )\n",
    ")\n",
    "- supposons que nous aayont 10 millions de texte et que le mot chat apparaissent dans 10 d'entre eux\n",
    "- la frequence inverse de document est  : \n",
    "log(10000000/1000)\\\n",
    "- Ainsi le poids TF_IDF est le produits des quantités TF et IDF "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application de la vectorisation :\n",
    "Soit l'ensemnle de textes suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_text = [\n",
    "    \"one geek helps two geeks\" ,\n",
    "    'two geeks help for geeks' ,\n",
    "    'fach geek helps many other geeks at geeksforgeeks'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le modules CountVectorizer de la librairie scikit-learn est un exelent outil pour transformer du texte en vecteur sur la base de la frequence des mots qui apparent dans l'ensemble du texte.\n",
    "- Ceci est utilie lorsque nous avons plusieurs texte et qu nous souhaitons convertir chaque mot de chaque texte en vecteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "Vec = CountVectorizer()\n",
    "Vec.fit(list_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at' 'fach' 'for' 'geek' 'geeks' 'geeksforgeeks' 'help' 'helps' 'many'\n",
      " 'one' 'other' 'two']\n",
      "{'one': 9, 'geek': 3, 'helps': 7, 'two': 11, 'geeks': 4, 'help': 6, 'for': 2, 'fach': 1, 'many': 8, 'other': 10, 'at': 0, 'geeksforgeeks': 5}\n"
     ]
    }
   ],
   "source": [
    "print(Vec.get_feature_names_out())\n",
    "print(Vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = Vec.transform(list_text) # valeurs adjuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 11)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 6)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 8)\t1\n",
      "  (2, 10)\t1\n",
      "[[0 0 0 1 1 0 0 1 0 1 0 1]\n",
      " [0 0 1 0 2 0 1 0 0 0 0 1]\n",
      " [1 1 0 1 1 1 0 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vec) ; \n",
    "# affichage de la matrce creuse\n",
    "print(vec.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etape 3; Calcule des ponderation TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t0.43306684852870914\n",
      "  (0, 9)\t0.5694308628404254\n",
      "  (0, 7)\t0.43306684852870914\n",
      "  (0, 4)\t0.33631504064053513\n",
      "  (0, 3)\t0.43306684852870914\n",
      "  (1, 11)\t0.3815187681027303\n",
      "  (1, 6)\t0.5016513317715935\n",
      "  (1, 4)\t0.5925667154413486\n",
      "  (1, 2)\t0.5016513317715935\n",
      "  (2, 10)\t0.3920626253314354\n",
      "  (2, 8)\t0.3920626253314354\n",
      "  (2, 7)\t0.298173732156414\n",
      "  (2, 5)\t0.3920626253314354\n",
      "  (2, 4)\t0.23155850231624595\n",
      "  (2, 3)\t0.298173732156414\n",
      "  (2, 1)\t0.3920626253314354\n",
      "  (2, 0)\t0.3920626253314354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(vec)\n",
    "\n",
    "tfidf = tfidf.transform(vec)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               TFIDF-T1  TFIDF-T2  TFIDF-T3\n",
      "at             0.000000  0.000000  0.392063\n",
      "fach           0.000000  0.000000  0.392063\n",
      "for            0.000000  0.501651  0.000000\n",
      "geek           0.433067  0.000000  0.298174\n",
      "geeks          0.336315  0.592567  0.231559\n",
      "geeksforgeeks  0.000000  0.000000  0.392063\n",
      "help           0.000000  0.501651  0.000000\n",
      "helps          0.433067  0.000000  0.298174\n",
      "many           0.000000  0.000000  0.392063\n",
      "one            0.569431  0.000000  0.000000\n",
      "other          0.000000  0.000000  0.392063\n",
      "two            0.433067  0.381519  0.000000\n"
     ]
    }
   ],
   "source": [
    "feature_names = Vec.get_feature_names_out()\n",
    "list_vector = pd.DataFrame(tfidf.T.todense() , index=feature_names,columns=['TFIDF-T1','TFIDF-T2',\"TFIDF-T3\"])\n",
    "print(list_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TFIDF-T1</th>\n",
       "      <th>TFIDF-T2</th>\n",
       "      <th>TFIDF-T3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.569431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geek</th>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helps</th>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>0.433067</td>\n",
       "      <td>0.381519</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geeks</th>\n",
       "      <td>0.336315</td>\n",
       "      <td>0.592567</td>\n",
       "      <td>0.231559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fach</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geeksforgeeks</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501651</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TFIDF-T1  TFIDF-T2  TFIDF-T3\n",
       "one            0.569431  0.000000  0.000000\n",
       "geek           0.433067  0.000000  0.298174\n",
       "helps          0.433067  0.000000  0.298174\n",
       "two            0.433067  0.381519  0.000000\n",
       "geeks          0.336315  0.592567  0.231559\n",
       "at             0.000000  0.000000  0.392063\n",
       "fach           0.000000  0.000000  0.392063\n",
       "for            0.000000  0.501651  0.000000\n",
       "geeksforgeeks  0.000000  0.000000  0.392063\n",
       "help           0.000000  0.501651  0.000000\n",
       "many           0.000000  0.000000  0.392063\n",
       "other          0.000000  0.000000  0.392063"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_vector = list_vector.sort_values(by=[\"TFIDF-T1\"] , ascending=False)\n",
    "list_vector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LA NORMALISATION :\n",
    "il s'agit de normaliser des vecteur a la longueur pour faire abstraction de la longuer  du texte d'origine (norme 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
