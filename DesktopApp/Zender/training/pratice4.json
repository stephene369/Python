[
    {
        "question": "1. Question\nIn your AWS VPC, you need to add a new subnet that will allow you to host a total of 20 EC2 instances.\nWhich of the following IPv4 CIDR blocks can you use for this scenario?\n172.0.0.0/30\n172.0.0.0/27\n172.0.0.0/28\n172.0.0.0/29\n",
        "answer": [
            2
        ],
        "explanation": "When you create a VPC, you must specify an IPv4 CIDR block for the VPC\nThe allowed block size is between a /16 netmask (65,536 IP addresses) and /28 netmask (16 IP addresses)\nThe CIDR block must not overlap with any existing CIDR block that\u2019s associated with the VPC\nA /27 subnet mask provides 32 addresses\nThe first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance\nThe following list shows total addresses for different subnet masks: /32 = 1 ; /31 = 2 ; /30 = 4 ; /29 = 8 ; /28 = 16 ; /27 = 32\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/"
    },
    {
        "question": "2. Question\nYou have a requirement to perform a large-scale testing operation that will assess the ability of your application to scale. You are planning on deploying a large number of c3.2xlarge instances with several PIOPS EBS volumes attached to each. You need to ensure you don\u2019t run into any problems with service limits. What are the service limits you need to be aware of in this situation?\n20 On-Demand EC2 instances and 100,000 aggregate PIOPS per account\n20 On-Demand EC2 instances and 100,000 aggregate PIOPS per region\n20 On-Demand EC2 instances and 300 TiB of aggregate PIOPS volume storage per region\n20 On-Demand EC2 instances and 300 TiB of aggregate PIOPS volume storage per account\n",
        "answer": [
            3
        ],
        "explanation": "You are limited to running up to a total of 20 On-Demand instances across the instance family, purchasing 20 Reserved Instances, and requesting Spot Instances per your dynamic spot limit per region (by default)\nYou are limited to an aggregate of 300 TiB of aggregate PIOPS volumes per region and 300,000 aggregate PIOPS\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ec2/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/"
    },
    {
        "question": "3. Question\nYou are discussing EC2 with a colleague and need to describe the differences between EBS-backed instances and Instance store-backed instances. Which of the statements below would be valid descriptions? (choose 2)\nFor both types of volume rebooting the instances will result in data loss\nOn an EBS-backed instance, the default action is for the root EBS volume to be deleted upon termination\nBy default, root volumes for both types will be retained on termination unless you configured otherwise\nEBS volumes can be detached and reattached to other EC2 instances\nInstance store volumes can be detached and reattached to other EC2 instances\n",
        "answer": [
            2,
            4
        ],
        "explanation": "On an EBS-backed instance, the default action is for the root EBS volume to be deleted upon termination\nEBS volumes can be detached and reattached to other EC2 instances\nInstance store volumes cannot be detached and reattached to other EC2 instances\nWhen rebooting the instances for both types data will not be lost\nBy default, root volumes for both types will be deleted on termination unless you configured otherwise\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/"
    },
    {
        "question": "4. Question\nYou are creating a series of environments within a single VPC. You need to implement a system of categorization that allows for identification of EC2 resources by business unit, owner, or environment.\nWhich AWS feature allows you to do this?\nMetadata\nParameters\nTags\nCustom filters\n",
        "answer": [
            3
        ],
        "explanation": "A tag is a label that you assign to an AWS resource. Each tag consists of a key and an optional value, both of which you define. Tags enable you to categorize your AWS resources in different ways, for example, by purpose, owner, or environment\nInstance metadata is data about your instance that you can use to configure or manage the running instance\nParameters and custom filters are not used for categorization\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ec2/"
    },
    {
        "question": "5. Question\nYour manager is interested in reducing operational overhead and cost and heard about \u201cserverless\u201d computing at a conference he recently attended. He has asked you if AWS provide any services that the company can leverage. Which services from the list below would you tell him about? (choose 2)\nAPI Gateway\nEC2\nEMR\nECS\nLambda\n",
        "answer": [
            1,
            5
        ],
        "explanation": "AWS Serverless services include (but not limited to):\nAPI Gateway\nLambda\nS3\nDynamoDB\nSNS\nSQS\nKinesis\nEMR, EC2 and ECS all use compute instances running on Amazon EC2 so are not serverless\nReferences:\nhttps://aws.amazon.com/serverless/"
    },
    {
        "question": "6. Question\nAn application you are designing will gather data from a website hosted on an EC2 instance and write the data to an S3 bucket. The application will use API calls to interact with the EC2 instance and S3 bucket.\nWhich Amazon S3 access control method will be the the MOST operationally efficient? (choose 2)\nGrant AWS Management Console access\nCreate an IAM policy\nCreate a bucket policy\nUse key pairs\nGrant programmatic access\n",
        "answer": [
            2,
            5
        ],
        "explanation": "Policies are documents that define permissions and can be applied to users, groups and roles. Policy documents are written in JSON (key value pair that consists of an attribute and a value)\nWithin an IAM policy you can grant either programmatic access or AWS Management Console access to Amazon S3 resources\nKey pairs are used for access to EC2 instances; a bucket policy would not assist with access control with EC2 and granting management console access will not assist the application which is making API calls to the services\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "7. Question\nYou are designing the disk configuration for an EC2 instance. The instance will be running an application that requires heavy read/write IOPS. You need to provision a single volume that is 500 GiB in size and needs to support 20,000 IOPS.\nWhat EBS volume type will you select?\nEBS General Purpose SSD in a RAID 1 configuration\nEBS General Purpose SSD\nEBS Provisioned IOPS SSD\nEBS Throughput Optimized HDD\n",
        "answer": [
            3
        ],
        "explanation": "This is simply about understanding the performance characteristics of the different EBS volume types. The only EBS volume type that supports over 10,000 IOPS is Provisioned IOPS SSD\nSSD, General Purpose \u2013 GP2\nBaseline of 3 IOPS per GiB with a minimum of 100 IOPS\nBurst up to 3000 IOPS for volumes >= 334GB)\nSSD, Provisioned IOPS \u2013 I01\nMore than 10,000 IOPS\nUp to 32000 IOPS per volume\nUp to 50 IOPS per GiB\nHDD, Throughput Optimized \u2013 (ST1)\nThroughput measured in MB/s, and includes the ability to burst up to 250 MB/s per TB, with a baseline throughput of 40 MB/s per TB and a maximum throughput of 500 MB/s per volume\nHDD, Cold \u2013 (SC1)\nLowest cost storage \u2013 cannot be a boot volume\nThese volumes can burst up to 80 MB/s per TB, with a baseline throughput of 12 MB/s per TB and a maximum throughput of 250 MB/s per volume\nHDD, Magnetic \u2013 Standard \u2013 cheap, infrequently accessed storage \u2013 lowest cost storage that can be a boot volume\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/"
    },
    {
        "question": "8. Question\nAn Auto Scaling group is configured with the default termination policy. The group spans multiple Availability Zones and each AZ has the same number of instances running.\nA scale in event needs to take place, what is the first step in evaluating which instances to terminate?\nSelect instances that use the oldest launch configuration\nSelect instances randomly\nSelect the newest instance in the group\nSelect instances that are closest to the next billing hour\n",
        "answer": [
            1
        ],
        "explanation": "Using the default termination policy, when there are even number of instances in multiple AZs, Auto Scaling will first select the instances with the oldest launch configuration, and if multiple instances share the oldest launch configuration, AS then selects the instances that are closest to the next billing hour\nPlease see the AWS article linked below for more details on the termination process\nReferences:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html"
    },
    {
        "question": "9. Question\nYou need to connect from your office to a Linux instance that is running in a public subnet in your VPC using the Internet. Which of the following items are required to enable this access? (choose 2)\nA bastion host\nA Public or Elastic IP address on the EC2 instance\nA NAT Gateway\nAn IPSec VPN\nAn Internet Gateway attached to the VPC and route table attached to the public subnet pointing to it\n",
        "answer": [
            2,
            5
        ],
        "explanation": "A public subnet is a subnet that has an Internet Gateway attached and \u201cEnable auto-assign public IPv4 address\u201d enabled. Instances require a public IP or Elastic IP address. It is also necessary to have the subnet route table updated to point to the Internet Gateway and security groups and network ACLs must be configured to allow the SSH traffic on port 22\nA bastion host can be used to access instances in private subnets but is not required for instances in public subnets\nA NAT Gateway allows instances in private subnets to access the Internet, it is not used for remote access\nAn IPSec VPN is not required to connect to an instance in a public subnet\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/"
    },
    {
        "question": "10. Question\nA customer has a public-facing web application hosted on a single Amazon Elastic Compute Cloud (EC2) instance serving videos directly from an Amazon S3 bucket. Which of the following will restrict third parties from directly accessing the video assets in the bucket?\nUse a bucket policy to only allow the public IP address of the Amazon EC2 instance hosting the customer website\nUse a bucket policy to only allow referrals from the main website URL\nLaunch the website Amazon EC2 instance using an IAM role that is authorized to access the videos\nRestrict access to the bucket to the public CIDR range of the company locations\n",
        "answer": [
            2
        ],
        "explanation": "To allow read access to the S3 video assets from the public-facing web application, you can add a bucket policy that allows s3:GetObject permission with a condition, using the aws:referer key, that the get request must originate from specific webpages. This is a good answer as it fully satisfies the objective of ensuring the that EC2 instance can access the videos but direct access to the videos from other sources is prevented.\nYou can use condition statements in a bucket policy to restrict access via IP address. However, using the referrer condition in a bucket policy is preferable as it is a best practice to use DNS names / URLs instead of hard-coding IPs whenever possible\nRestricting access to the bucket to the public CIDR range of the company locations will stop third-parties from accessing the bucket however it will also stop the EC2 instance from accessing the bucket and the question states that the EC2 instance is serving the files directly\nLaunching the EC2 instance with an IAM role that is authorized to access the videos is only half a solution as you would also need to create a bucket policy that specifies that the IAM role is granted access\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-4"
    },
    {
        "question": "11. Question\nYou are a Solutions Architect for a pharmaceutical company. The company uses a strict process for release automation that involves building and testing services in 3 separate VPCs. A peering topology is configured with VPC-A peered with VPC-B and VPC-B peered with VPC-C. The development team wants to modify the process so that they can release code directly from VPC-A to VPC-C.\nHow can this be accomplished?\nUpdate VPC-As route table with an entry using the VPC peering as a target\nCreate a new VPC peering connection between VPC-A and VPC-C\nUpdate VPC-Bs route table with peering targets for VPC-A and VPC-C and enable route propagation\nUpdate the CIDR blocks to match to enable inter-VPC routing\n",
        "answer": [
            2
        ],
        "explanation": "It is not possible to use transitive peering relationships with VPC peering and therefore you must create an additional VPC peering connection between VPC-A and VPC-C\nYou must update route tables to configure routing however updating VPC-As route table alone will not lead to the desired result without first creating the additional peering connection\nRoute propagation cannot be used to extend VPC peering connections\nYou cannot have matching (overlapping) CIDR blocks with VPC peering\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/"
    },
    {
        "question": "12. Question\nYou have an Amazon RDS Multi-AZ deployment across two availability zones. An outage of the availability zone in which the primary RDS DB instance is running occurs. What actions will take place in this circumstance? (choose 2)\nA manual failover of the DB instance will need to be initiated using Reboot with failover\nDue to the loss of network connectivity the process to switch to the standby replica cannot take place\nThe primary DB instance will switch over automatically to the standby replica\nA failover will take place once the connection draining timer has expired\nThe failover mechanism automatically changes the DNS record of the DB instance to point to the standby DB instance\n",
        "answer": [
            3,
            5
        ],
        "explanation": "Multi-AZ RDS creates a replica in another AZ and synchronously replicates to it (DR only)\nA failover may be triggered in the following circumstances:\nLoss of primary AZ or primary DB instance failure\nLoss of network connectivity on primary\nCompute (EC2) unit failure on primary\nStorage (EBS) unit failure on primary\nThe primary DB instance is changed\nPatching of the OS on the primary DB instance\nManual failover (reboot with failover selected on primary)\nDuring failover RDS automatically updates configuration (including DNS endpoint) to use the second node\nThe process to failover is not reliant on network connectivity as it is designed for fault tolerance\nConnection draining timers are applicable to ELBs not RDS\nYou do not need to manually failover the DB instance, multi-AZ has an automatic process as outlined above\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-rds/"
    },
    {
        "question": "13. Question\nYou need to record connection information from clients using an ELB. When enabling the Proxy Protocol with an ELB to carry connection information from the source requesting the connection, what prerequisites apply? (choose 2)\nConfirm that your load balancer is configured to include the X-Forwarded-For request header\nConfirm that your instances are on-demand instances\nConfirm that your back-end listeners are configured for TCP and front-end listeners are configured for TCP\nConfirm that your load balancer is not behind a proxy server with Proxy Protocol enabled\nConfirm that your load balancer is using HTTPS listeners\n",
        "answer": [
            3,
            4
        ],
        "explanation": "Proxy protocol for TCP/SSL carries the source (client) IP/port information. The Proxy Protocol header helps you identify the IP address of a client when you have a load balancer that uses TCP for back-end connections. You need to ensure the client doesn\u2019t go through a proxy or there will be multiple proxy headers. You also need to ensure the EC2 instance\u2019s TCP stack can process the extra information\nThe back-end and front-end listeners must be configured for TCP\nHTTPS listeners do not carry proxy protocol information (use the X-Forwarded-For header instead)\nIt doesn\u2019t matter what type of pricing model you\u2019re using for EC2 (e.g. on-demand, reserved etc.)\nX-Forwarded-For is a different protocol that operates at layer 7 whereas proxy protocol operates at layer 4\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/using-elb-listenerconfig-quickref.html"
    },
    {
        "question": "14. Question\nYou have been assigned the task of moving some sensitive documents into the AWS cloud. You need to ensure that the security of the documents is maintained. Which AWS features can help ensure that the sensitive documents cannot be read even if they are compromised? (choose 2)\nEBS encryption with Customer Managed Keys\nS3 Server-Side Encryption\nS3 cross region replication\nIAM Access Policy\nEBS snapshots\n",
        "answer": [
            1,
            2
        ],
        "explanation": "It is not specified what types of documents are being moved into the cloud or what services they will be placed on. Therefore, we can assume that options include S3 and EBS. To prevent the documents from being read if they are compromised, we need to encrypt them. Both of these services provide native encryption functionality to ensure security of the sensitive documents. With EBS you can use KMS-managed or customer-managed encryption keys. With S3 you can use client-side or server-side encryption\nIAM access policies can be used to control access but if the documents are somehow compromised, they will not stop the documents from being read. For this we need encryption, and IAM access policies  are not used for controlling encryption\nEBS snapshots are used for creating a point-in-time backup or data. They do maintain the encryption status of the data from the EBS volume but are not used for actually encrypting the data in the first place\nS3 cross-region replication can be used for fault tolerance but does not apply any additional security to the data\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "15. Question\nAn Auto Scaling Group is unable to respond quickly enough to load changes resulting in lost messages from another application tier. The messages are typically around 128KB in size.\nWhat is the best design option to prevent the messages from being lost?\nUse larger EC2 instance sizes\nLaunch an Elastic Load Balancer\nStore the messages on an SQS queue\nStore the messages on Amazon S3\n",
        "answer": [
            3
        ],
        "explanation": "In this circumstance the ASG cannot launch EC2 instances fast enough. You need to be able to store the messages somewhere so they don\u2019t get lost whilst the EC2 instances are launched. This is a classic use case for decoupling and SQS is designed for exactly this purpose\nAmazon Simple Queue Service (Amazon SQS) is a web service that gives you access to message queues that store messages waiting to be processed. SQS offers a reliable, highly-scalable, hosted queue for storing messages in transit between computers. An SQS queue can be used to create distributed/decoupled applications\nStoring the messages on S3 is potentially feasible but SQS is the preferred solution as it is designed for decoupling. If the messages are over 256KB and therefore cannot be stored in SQS, you may want to consider using S3 and it can be used in combination with SQS by using the Amazon SQS Extended Client Library for Java\nAn ELB can help to distribute incoming connections to the back-end EC2 instances however if the ASG is not scaling fast enough then there aren\u2019t enough resources for the ELB to distributed traffic to\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-sqs/"
    },
    {
        "question": "16. Question\nThere is a problem with an EC2 instance that was launched by AWS Auto Scaling. The EC2 status checks have reported that the instance is \u201cImpaired\u201d. What action will AWS Auto Scaling take?\nIt will mark the instance for termination, terminate it, and then launch a replacement\nAuto Scaling will wait for 300 seconds to give the instance a chance to recover\nIt will launch a new instance immediately and then mark the impaired one for replacement\nAuto Scaling performs its own status checks and does not integrate with EC2 status checks\n",
        "answer": [
            1
        ],
        "explanation": "If any health check returns an unhealthy status the instance will be terminated. Unlike AZ rebalancing, termination of unhealthy instances happens first, then Auto Scaling attempts to launch new instances to replace terminated instances\nAS will not launch a new instance immediately as it always terminates unhealthy instance before launching a replacement\nAuto Scaling does not wait for 300 seconds, once the health check has failed the configured number of times the instance will be terminated\nAuto Scaling does integrate with EC2 status checks as well as having its own status checks\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/"
    },
    {
        "question": "17. Question\nYou\u2019re trying to explain to a colleague typical use cases where you can use the Simple Workflow Service (SWF). Which of the scenarios below would be valid? (choose 2)\nFor web applications that require content delivery networks\nSending notifications via SMS when an EC2 instance reaches a certain threshold\nProviding a reliable, highly-scalable, hosted queue for storing messages in transit between EC2 instances\nManaging a multi-step and multi-decision checkout process for a mobile application\nCoordinating business process workflows across distributed application components\n",
        "answer": [
            4,
            5
        ],
        "explanation": "Amazon Simple Workflow Service (SWF) is a web service that makes it easy to coordinate work across distributed application components\nSWF enables applications for a range of use cases, including media processing, web application back-ends, business process workflows, and analytics pipelines, to be designed as a coordination of tasks\nYou should use Amazon SNS for sending SMS messages\nYou should use CloudFront if you need a CDN\nYo should use SQS for storing messages in a queue\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-swf/"
    },
    {
        "question": "18. Question\nYou work for a systems integrator running a platform that stores medical records. The government security policy mandates that patient data that contains personally identifiable information (PII) must be encrypted at all times, both at rest and in transit. You are using Amazon S3 to back up data into the AWS cloud.\nHow can you ensure the medical records are properly secured? (choose 2)\nBefore uploading the data to S3 over HTTPS, encrypt the data locally using your own encryption keys\nUpload the data using CloudFront with an EC2 origin\nAttach an encrypted EBS volume to an EC2 instance\nEnable Server Side Encryption with S3 managed keys on an S3 bucket using AES-128\nEnable Server Side Encryption with S3 managed keys on an S3 bucket using AES-256\n",
        "answer": [
            1,
            5
        ],
        "explanation": "When data is stored in an encrypted state it is referred to as encrypted \u201cat rest\u201d and when it is encrypted as it is being transferred over a network it is referred to as encrypted \u201cin transit\u201d. You can securely upload/download your data to Amazon S3 via SSL endpoints using the HTTPS protocol (In Transit \u2013 SSL/TLS). You have the option of encrypting the data locally before it is uploaded or uploading using SSL/TLS so it is secure in transit and encrypting on the Amazon S3 side using S3 managed keys. The S3 managed keys will be AES-256 (not AES-128) bit keys\nUploading data using CloudFront with an EC2 origin or using an encrypted EBS volume attached to an EC2 instance is not a solution to this problem as your company wants to backup these records onto S3 (not EC2/EBS)\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "19. Question\nYou are a Solutions Architect for Digital Cloud Training. A client is migrating a large amount of data that their customers access onto the AWS cloud. The client is located in Australia and most of their customers will be accessing the data from within Australia. The customer has asked you for some advice about S3 buckets.\nWhich of the following statements would be good advice? (choose 2)\nTo reduce latency and improve performance, create the buckets in the Asia Pacific (Sydney) region\nS3 is a global service so it doesn\u2019t matter where you create your buckets\nS3 is a universal namespace so bucket names must be unique globally\nBuckets can be renamed after they have been created\nS3 buckets have a limit on the number of objects you can store in them\n",
        "answer": [
            1,
            3
        ],
        "explanation": "For better performance, lower latency and lower costs the buckets should be created in the region that is closest to the client\u2019s customers\nS3 is a universal namespace so names must be unique globally\nBucket names cannot be changed after they have been created\nAn S3 bucket is created within a region and all replicated copies of the data stay within the region unless you explicitly configure cross-region replication\nThere is no limit on the number of objects you can store in an S3 bucket\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "20. Question\nAn application has been deployed in a private subnet within your VPC and an ELB will be used to accept incoming connections. You need to setup the configuration for the listeners on the ELB. When using a Classic Load Balancer, which of the following combinations of listeners support the proxy protocol? (choose 2)\nFront-End \u2013 TCP & Back-End \u2013 TCP\nFront-End \u2013 SSL & Back-End \u2013 SSL\nFront-End \u2013 SSL & Back-End \u2013 TCP\nFront-End \u2013 HTTP & Back-End SSL\nFront-End \u2013 TCP & Back-End SSL\n",
        "answer": [
            1,
            3
        ],
        "explanation": "The proxy protocol only applies to L4 and the back-end listener must be TCP for proxy protocol\nWhen using the proxy protocol the front-end listener can be either TCP or SSL\nThe X-forwarded-for header only applies to L7\nProxy protocol for TCP/SSL carries the source (client) IP/port information. The Proxy Protocol header helps you identify the IP address of a client when you have a load balancer that uses TCP for back-end connection\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/using-elb-listenerconfig-quickref.html"
    },
    {
        "question": "21. Question\nYou created a second ENI (eth1) interface when launching an EC2 instance. You would like to terminate the instance and have not made any changes.\nWhat will happen to the attached ENIs?\neth1 will be terminated, but eth0 will persist\nBoth eth0 and eth1 will be terminated with the instance\neth1 will persist but eth0 will be terminated\nBoth eth0 and eth1 will persist\n",
        "answer": [
            3
        ],
        "explanation": "By default, Eth0 is the only Elastic Network Interface (ENI) created with an EC2 instance when launched. You can add additional interfaces to EC2 instances (number dependent on instances family/type). Default interfaces are terminated with instance termination. Manually added interfaces are not terminated by default\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ec2/"
    },
    {
        "question": "22. Question\nAn application that you manage uses a combination of Reserved and On-Demand instances to handle typical load. The application involves performing analytics on a set of data and you need to temporarily deploy a large number of EC2 instances. You only need these instances to be available for a short period of time until the analytics job is completed.\nIf job completion is not time-critical what is likely to be the MOST cost-effective choice of EC2 instance type to use for this requirement?\nUse Reserved instances\nUse On-Demand instances\nUse Spot instances\nUse dedicated hosts\n",
        "answer": [
            3
        ],
        "explanation": "The key requirements here are that you need to temporarily deploy a large number of instances, can tolerate an delay (not time-critical), and need the most economical solution. In this case Spot instances are likely to be the most economical solution. You must be able to tolerate delays if using Spot instances as if the market price increases your instances will be terminated and you may have to wait for the price to lower back to your budgeted allowance.\nOn-demand is good for temporary deployments when you cannot tolerate any delays (instances being terminated by AWS). It is likely to be more expensive than Spot however so if delays can be tolerated it is not the best solution\nReserved instances are used for longer more stable requirements where you can get a discount for a fixed 1 or 3 year term. This pricing model is not good for temporary requirements\nAn EC2 Dedicated Host is a physical server with EC2 instance capacity fully dedicated to your use. They are much more expensive than on-demand or Spot instances and are used for use cases such as bringing your own socket-based software licences to AWS or for compliance reasons\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ec2/"
    },
    {
        "question": "23. Question\nA health club is developing a mobile fitness app that allows customers to upload statistics and view their progress. Amazon Cognito is being used for authentication, authorization and user management and users will sign-in with Facebook IDs.\nIn order to securely store data in DynamoDB, the design should use temporary AWS credentials. What feature of Amazon Cognito is used to obtain temporary credentials to access AWS services?\nSAML Identity Providers\nIdentity Pools\nUser Pools\nKey Pairs\n",
        "answer": [
            2
        ],
        "explanation": "With an identity pool, users can obtain temporary AWS credentials to access AWS services, such as Amazon S3 and DynamoDB\nA user pool is a user directory in Amazon Cognito. With a user pool, users can sign in to web or mobile apps through Amazon Cognito, or federate through a third-party identity provider (IdP)\nSAML Identity Providers are supported IDPs for identity pools but cannot be used for gaining temporary credentials for AWS services\nKey pairs are used in Amazon EC2 for access to instances\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "24. Question\nA membership website your company manages has become quite popular and is gaining members quickly. The website currently runs on EC2 instances with one web server instance and one DB instance running MySQL. You are concerned about the lack of high-availability in the current architecture.\nWhat can you do to easily enable HA without making major changes to the architecture?\nInstall MySQL on an EC2 instance in another AZ and enable replication\nInstall MySQL on an EC2 instance in the same AZ and enable replication\nEnable Multi-AZ for the MySQL instance\nCreate a Read Replica in another AZ\n",
        "answer": [
            1
        ],
        "explanation": "If you are installing MySQL on an EC2 instance you cannot enable read replicas or multi-AZ. Instead you would need to use Amazon RDS with a MySQL DB engine to use these features\nMigrating to RDS would entail a major change to the architecture so is not really feasible. In this example it will therefore be easier to use the native HA features of MySQL rather than to migrate to RDS. You would want to place the second MySQL DB instance in another AZ to enable high availability and fault tolerance\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ec2/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-rds/"
    },
    {
        "question": "25. Question\nYou are a Developer working for Digital Cloud Training. You are planning to write some code that creates a URL that lets users who sign in to your organization\u2019s network securely access the AWS Management Console. The URL will include a sign-in token that you get from AWS that authenticates the user to AWS. You are using Microsoft Active Directory Federation Services as your identity provider (IdP) which is compatible with SAML 2.0.\nWhich of the steps below will you need to include when developing your custom identity broker? (choose 2)\nGenerate a pre-signed URL programmatically using the AWS SDK for Java or the AWS SDK for .NET\nDelegate access to the IdP through the \"Configure Provider\" wizard in the IAM console\nAssume an IAM Role through the console or programmatically with the AWS CLI, Tools for Windows PowerShell or API\nCall the AWS federation endpoint and supply the temporary security credentials to request a sign-in token\nCall the AWS Security Token Service (AWS STS) AssumeRole or GetFederationToken API operations to obtain temporary security credentials for the user\n",
        "answer": [
            4,
            5
        ],
        "explanation": "The aim of this solution is to create a single sign-on solution that enables users signed in to the organization\u2019s Active Directory service to be able to connect to AWS resources. When developing a custom identity broker you use the AWS STS service.\nThe AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for IAM users or for users that you authenticate (federated users). The steps performed by the custom identity broker to sign users into the AWS management console are:\nVerify that the user is authenticated by your local identity system\nCall the AWS Security Token Service (AWS STS) AssumeRole or GetFederationToken API operations to obtain temporary security credentials for the user\nCall the AWS federation endpoint and supply the temporary security credentials to request a sign-in token\nConstruct a URL for the console that includes the token\nGive the URL to the user or invoke the URL on the user\u2019s behalf\nYou cannot generate a pre-signed URL for this purpose using SDKs, delegate access through the IAM console os directly assume IAM roles.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html"
    },
    {
        "question": "26. Question\nYou need to create an EBS volume to mount to an existing EC2 instance for an application that will be writing structured data to the volume. The application vendor suggests that the performance of the disk should be up to 3 IOPS per GB. You expect the capacity of the volume to grow to 2TB.\nTaking into account cost effectiveness, which EBS volume type would you select?\nProvisioned IOPS (Io1)\nGeneral Purpose (GP2)\nThroughput Optimized HDD (ST1)\nCold HDD (SC1)\n",
        "answer": [
            2
        ],
        "explanation": "SSD, General Purpose (GP2) provides enough IOPS to support this requirement and is the most economical option that does. Using Provisioned IOPS would be more expensive and the other two options do not provide an SLA for IOPS\nMore information on the volume types:\n\u2013         SSD, General Purpose (GP2) provides 3 IOPS per GB up to 16,000 IOPS. Volume size is 1 GB to 16 TB\n\u2013         Provisioned IOPS (Io1) provides the IOPS you assign up to 50 IOPS per GiB and up to 64,000 IOPS per volume. Volume size is 4 GB to 16TB\n\u2013         Throughput Optimized HDD (ST1) provides up to 500 IOPS per volume but does not provide an SLA for IOPS\n\u2013         Cold HDD (SC1) provides up to 250 IOPS per volume but does not provide an SLA for IOPS\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html?icmpid=docs_ec2_console"
    },
    {
        "question": "27. Question\nYour manager has asked you to explain the benefits of using IAM groups. Which of the below statements are valid benefits? (choose 2)\nProvide the ability to create custom permission policies\nEnables you to attach IAM permission policies to more than one user at a time\nGroups let you specify permissions for multiple users, which can make it easier to manage the permissions for those users\nProvide the ability to nest groups to create an organizational hierarchy\nYou can restrict access to the subnets in your VPC\n",
        "answer": [
            2,
            3
        ],
        "explanation": "Groups are collections of users and have policies attached to them. A group is not an identity and cannot be identified as a principal in an IAM policy. Use groups to assign permissions to users. Use the principal of least privilege when assigning permissions. You cannot nest groups (groups within groups)\nYou cannot use groups to restrict access to subnet in your VPC\nCustom permission policies are created using IAM policies. These are then attached to users, groups or roles\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/"
    },
    {
        "question": "28. Question\nYou are designing solutions that will utilize CloudFormation templates and your manager has asked how much extra will it cost to use CloudFormation to deploy resources?\nCloudFormation is charged per hour of usage\nAmazon charge a flat fee for each time you use CloudFormation\nThere is no additional charge for AWS CloudFormation, you only pay for the AWS resources that are created\nThe cost is based on the size of the template\n",
        "answer": [
            3
        ],
        "explanation": "There is no additional charge for AWS CloudFormation. You pay for AWS resources (such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation in the same manner as if you created them manually. You only pay for what you use, as you use it; there are no minimum fees and no required upfront commitments\nThere is no flat fee, per hour usage costs or charges applicable to templates\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/"
    },
    {
        "question": "29. Question\nAn important application you manage uses an Elastic Load Balancer (ELB) to distribute incoming requests amongst a fleet of EC2 instances. You need to ensure any operational issues are identified. Which of the statements below are correct about monitoring of an ELB? (choose 2)\nCloudWatch metrics can be logged to an S3 bucket\nAccess logs are enabled by default\nCloudTrail can be used to capture application logs\nAccess logs can identify requester, IP, and request type\nInformation is sent to CloudWatch every minute if there are active requests\n",
        "answer": [
            4,
            5
        ],
        "explanation": "Information is sent by the ELB to CloudWatch every 1 minute when requests are active. Can be used to trigger SNS notifications\nAccess Logs are disabled by default. Includes information about the clients (not included in CloudWatch metrics) including identifying the requester, IP, request type etc. Access logs can be optionally stored and retained in S3\nCloudWatch metrics for ELB cannot be logged directly to an S3 bucket. Instead you should use ELB access logs\nCloudTrail is used to capture API calls to the ELB and logs can be stored in an S3 bucket\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/"
    },
    {
        "question": "30. Question\nYou have created a new VPC and setup an Auto Scaling Group to maintain a desired count of 2 EC2 instances. The security team has requested that the EC2 instances be located in a private subnet. To distribute load, you have to also setup an Internet-facing Application Load Balancer (ALB).\nWith your security team\u2019s wishes in mind what else needs to be done to get this configuration to work? (choose 2)\nAttach an Internet Gateway to the private subnets\nAdd an Elastic IP address to each EC2 instance in the private subnet\nAdd a NAT gateway to the private subnet\nAssociate the public subnets with the ALB\nFor each private subnet create a corresponding public subnet in the same AZ\n",
        "answer": [
            4,
            5
        ],
        "explanation": "ELB nodes have public IPs and route traffic to the private IP addresses of the EC2 instances. You need one public subnet in each AZ where the ELB is defined and the private subnets are located\nAttaching an Internet gateway (which is done at the VPC level, not the subnet level) or a NAT gateway will not assist as these are both used for outbound communications which is not the goal here\nELBs talk to the private IP addresses of the EC2 instances so adding an Elastic IP address to the instance won\u2019t help. Additionally Elastic IP addresses are used in public subnets to allow Internet access via an Internet Gateway\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/\nhttps://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/"
    },
    {
        "question": "31. Question\nA Solutions Architect is creating a design for a multi-tiered serverless application. Which two services form the application facing services from the AWS serverless infrastructure? (choose 2)\nAmazon ECS\nAPI Gateway\nElastic Load Balancer\nAWS Cognito\nAWS Lambda\n",
        "answer": [
            2,
            5
        ],
        "explanation": "The only application services here are API Gateway and Lambda and these are considered to be serverless services\nECS provides the platform for running containers and uses Amazon EC2 instances\nELB provides distribution of incoming network connections and also uses Amazon EC2 instances\nAWS Cognito is used for providing authentication services for web and mobile apps\nReferences:\nhttps://aws.amazon.com/serverless/"
    },
    {
        "question": "32. Question\nAn application you manage stores encrypted data in S3 buckets. You need to be able to query the encrypted data using SQL queries and write the encrypted results back the S3 bucket. As the data is sensitive you need to implement fine-grained control over access to the S3 bucket.\nWhat combination of services represent the BEST options support these requirements? (choose 2)\nUse bucket ACLs to restrict access to the bucket\nUse IAM policies to restrict access to the bucket\nUse AWS Glue to extract the data, analyze it, and load it back to the S3 bucket\nUse Athena for querying the data and writing the results back to the bucket\nUse the AWS KMS API to query the encrypted data, and the S3 API for writing the results\n",
        "answer": [
            2,
            4
        ],
        "explanation": "Athena also allows you to easily query encrypted data stored in Amazon S3 and write encrypted results back to your S3 bucket. Both, server-side encryption and client-side encryption are supported\nWith IAM policies, you can grant IAM users fine-grained control to your S3 buckets, and is preferable to using bucket ACLs\nAWS Glue is an ETL service and is not used for querying and analyzing data in S3\nThe AWS KMS API can be used for encryption purposes, however it cannot perform analytics so is not suitable\nReferences:\nhttps://aws.amazon.com/athena/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/"
    },
    {
        "question": "33. Question\nA new application you are designing will store data in an Amazon Aurora MySQL DB. You are looking for a way to enable inter-region disaster recovery capabilities with fast replication and fast failover. Which of the following options is the BEST solution?\nUse Amazon Aurora Global Database\nEnable Multi-AZ for the Aurora DB\nCreate a cross-region Aurora Read Replica\nCreate an EBS backup of the Aurora volumes and use cross-region replication to copy the snapshot\n",
        "answer": [
            1
        ],
        "explanation": "Amazon Aurora Global Database is designed for globally distributed applications, allowing a single Amazon Aurora database to span multiple AWS regions. It replicates your data with no impact on database performance, enables fast local reads with low latency in each region, and provides disaster recovery from region-wide outages. Aurora Global Database uses storage-based replication with typical latency of less than 1 second, using dedicated infrastructure that leaves your database fully available to serve application workloads. In the unlikely event of a regional degradation or outage, one of the secondary regions can be promoted to full read/write capabilities in less than 1 minute.\nYou can create an Amazon Aurora MySQL DB cluster as a Read Replica in a different AWS Region than the source DB cluster. Taking this approach can improve your disaster recovery capabilities, let you scale read operations into an AWS Region that is closer to your users, and make it easier to migrate from one AWS Region to another. However, this solution would not provide the fast storage replication and fast failover capabilities of the Aurora Global Database and is therefore not the best option\nEnabling Multi-AZ for the Aurora DB would provide AZ-level resiliency within the region not across regions\nThough you can take a DB snapshot and replicate it across regions, it does not provide an automated solution and it would not enable fast failover\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-rds/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Replication.html"
    },
    {
        "question": "34. Question\nYou are putting together a design for a web-facing application. The application will be run on EC2 instances behind ELBs in multiple regions in an active/passive configuration. The website address the application runs on is digitalcloud.training. You will be using Route 53 to perform DNS resolution for the application.\nHow would you configure Route 53 in this scenario based on AWS best practices? (choose 2)\nUse a Failover Routing Policy\nConnect the ELBs using CNAME records\nUse a Weighted Routing Policy\nSet Evaluate Target Health to \u201cNo\u201d for the primary\nConnect the ELBs using Alias records\n",
        "answer": [
            1,
            5
        ],
        "explanation": "The failover routing policy is used for active/passive configurations. Alias records can be used to map the domain apex (digitalcloud.training) to the Elastic Load Balancers.\nWeighted routing is not an active/passive routing policy. All records are active and the traffic is distributed according to the weighting\nYou cannot use CNAME records for the domain apex record, you must use Alias records\nFor Evaluate Target Health choose Yes for your primary record and choose No for your secondary record. For your primary record choose Yes for Associate with Health Check. Then for Health Check to Associate select the health check that you created for your primary resource\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-route-53/"
    },
    {
        "question": "35. Question\nYou recently noticed that your Network Load Balancer (NLB) in one of your VPCs is not distributing traffic evenly between EC2 instances in your AZs. There are an odd number of EC2 instances spread across two AZs. The NLB is configured with a TCP listener on port 80 and is using active health checks.\nWhat is the most likely problem?\nNLB can only load balance within a single AZ\nThere is no HTTP listener\nHealth checks are failing in one AZ due to latency\nCross-zone load balancing is disabled\n",
        "answer": [
            4
        ],
        "explanation": "Without cross-zone load balancing enabled, the NLB will distribute traffic 50/50 between AZs. As there are an odd number of instances across the two AZs some instances will not receive any traffic. Therefore enabling cross-zone load balancing will ensure traffic is distributed evenly between available instances in all AZs\nIf health checks fail this will cause the NLB to stop sending traffic to these instances. However, the health check packets are very small and it is unlikely that latency would be the issue within a region\nListeners are used to receive incoming connections. An NLB listens on TCP not on HTTP therefore having no HTTP listener is not the issue here\nAn NLB can load balance across multiple AZs just like the other ELB types\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/"
    },
    {
        "question": "36. Question\nYou are a Solutions Architect at Digital Cloud Training. A new client who has not used cloud computing has asked you to explain how AWS works. The client wants to know what service is provided that will provide a virtual network infrastructure that loosely resembles a traditional data center but has the capacity to scale more easily?\nElastic Load Balancing\nElastic Compute Cloud\nDirect Connect\nVirtual Private Cloud\n",
        "answer": [
            4
        ],
        "explanation": "Amazon VPC lets you provision a logically isolated section of the Amazon Web Services (AWS) cloud where you can launch AWS resources in a virtual network that you define. It is analogous to having your own DC inside AWS and provides complete control over the virtual networking environment including selection of IP ranges, creation of subnets, and configuration of route tables and gateways. A VPC is logically isolated from other VPCs on AWS\nElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions\nAmazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud\nAWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/"
    },
    {
        "question": "37. Question\nYou manage an application that uses Auto Scaling. Recently there have been incidents of multiple scaling events in an hour and you are looking at methods of stabilizing the Auto Scaling Group. Select the statements below that are correct with regards to the Auto Scaling cooldown period? (choose 2)\nThe default value is 600 seconds\nIt ensures that the Auto Scaling group terminates the EC2 instances that are least busy\nIt ensures that before the Auto Scaling group scales out, the EC2 instances can apply system updates\nThe default value is 300 seconds\nIt ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect\n",
        "answer": [
            4,
            5
        ],
        "explanation": "The cooldown period is a configurable setting for your Auto Scaling group that helps to ensure that it doesn\u2019t launch or terminate additional instances before the previous scaling activity takes effect\nThe default cooldown period is applied when you create your Auto Scaling group\nThe default value is 300 seconds\nYou can configure the default cooldown period when you create the Auto Scaling group, using the AWS Management Console, the create-auto-scaling-group command (AWS CLI), or the CreateAutoScalingGroup API operation\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/Cooldown.html"
    },
    {
        "question": "38. Question\nAn application you manage in your VPC uses an Auto Scaling Group that spans 3 AZs and there are currently 4 EC2 instances running in the group. What actions will Auto Scaling take, by default, if it needs to terminate an EC2 instance? (choose 2)\nWait for the cooldown period and then terminate the instance that has been running the longest\nRandomly select one of the 3 AZs, and then terminate an instance in that AZ\nTerminate an instance in the AZ which currently has 2 running EC2 instances\nSend an SNS notification, if configured to do so\nTerminate the instance with the least active network connections. If multiple instances meet this criterion, one will be randomly selected\n",
        "answer": [
            3,
            4
        ],
        "explanation": "Auto Scaling can perform rebalancing when it finds that the number of instances across AZs is not balanced. Auto Scaling rebalances by launching new EC2 instances in the AZs that have fewer instances first, only then will it start terminating instances in AZs that had more instances\nAuto Scaling can be configured to send an SNS email when:\n\u2013         An instance is launched\n\u2013         An instance is terminated\n\u2013         An instance fails to launch\n\u2013         An instance fails to terminate\nAuto Scaling does not terminate the instance that has been running the longest\nAuto Scaling will only terminate an instance randomly after it has first gone through several other selection steps. Please see the AWS article below for detailed information on the process\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html"
    },
    {
        "question": "39. Question\nYou are creating a CloudFormation template that will provision a new EC2 instance and new EBS volume. What do you need to specify to associate the block store with the instance?\nBoth the EC2 logical ID and the EBS logical ID\nThe EC2 logical ID\nBoth the EC2 physical ID and the EBS physical ID\nThe EC2 physical ID\n",
        "answer": [
            1
        ],
        "explanation": "Logical IDs are used to reference resources within the template\nPhysical IDs identify resources outside of AWS CloudFormation templates, but only after the resources have been created\nReferences:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resources-section-structure.html\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/"
    },
    {
        "question": "40. Question\nYou regularly launch EC2 instances manually from the console and want to streamline the process to reduce administrative overhead. Which feature of EC2 allows you to store settings such as AMI ID, instance type, key pairs and Security Groups?\nRun Command\nLaunch Templates\nLaunch Configurations\nPlacement Groups\n",
        "answer": [
            2
        ],
        "explanation": "Launch templates enable you to store launch parameters so that you do not have to specify them every time you launch an instance. When you launch an instance using the Amazon EC2 console, an AWS SDK, or a command line tool, you can specify the launch template to use\nLaunch Configurations are used with Auto Scaling Groups\nRun Command automates common administrative tasks, and lets you perform ad hoc configuration changes at scale\nYou can launch or start instances in a placement group, which determines how instances are placed on underlying hardware\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-launch-templates.html"
    },
    {
        "question": "41. Question\nYou are configuring Route 53 for a customer\u2019s website. Their web servers are behind an Internet-facing ELB. What record set would you create to point the customer\u2019s DNS zone apex record at the ELB?\nCreate an A record that is an Alias, and select the ELB DNS as a target\nCreate a PTR record pointing to the DNS name of the load balancer\nCreate an A record pointing to the DNS name of the load balancer\nCreate a CNAME record that is an Alias, and select the ELB DNS as a target\n",
        "answer": [
            1
        ],
        "explanation": "An Alias record can be used for resolving apex or naked domain names (e.g. example.com). You can create an A record that is an Alias that uses the customer\u2019s website zone apex domain name and map it to the ELB DNS name\nA CNAME record can\u2019t be used for resolving apex or naked domain names\nA standard A record maps the DNS domain name to the IP address of a resource. You cannot obtain the IP of the ELB so you must use an Alias record which maps the DNS domain name of the customer\u2019s website to the ELB DNS name (rather than its IP)\nPTR records are reverse lookup records where you use the IP to find the DNS name\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-route-53/"
    },
    {
        "question": "42. Question\nAn Auto Scaling Group in which you have four EC2 instances running is becoming heavily loaded. The instances are using the m4.large instance type and the CPUs are hitting 80%. Due to licensing constraints you don\u2019t want to add additional instances to the ASG so you are planning to upgrade to the m4.xlarge instance type instead. You need to make the change immediately but don\u2019t want to terminate the existing instances.\nHow can you perform the change without causing the ASG to launch new instances? (choose 2)\nEdit the existing launch configuration and specify the new instance type\nOn the ASG suspend the Auto Scaling process until you have completed the change\nStop each instance and change its instance type. Start the instance again\nChange the instance type and then restart the instance\nCreate a new launch configuration with the new instance type specified\n",
        "answer": [
            2,
            3
        ],
        "explanation": "When you resize an instance, you must select an instance type that is compatible with the configuration of the instance. You must stop your Amazon EBS\u2013backed instance before you can change its instance type\nYou can suspend and then resume one or more of the scaling processes for your Auto Scaling group. Suspending scaling processes can be useful when you want to investigate a configuration problem or other issue with your web application and then make changes to your application, without invoking the scaling processes\nYou do not need to create a new launch configuration and you cannot edit an existing launch configuration\nYou cannot change an instance type without first stopping the instance\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-resize.html"
    },
    {
        "question": "43. Question\nAn issue has been raised to you whereby a client is concerned about the permissions assigned to his containerized applications. The containers are using the EC2 launch type. The current configuration uses the container instance\u2019s IAM roles for assigning permissions to the containerized applications.\nThe client has asked if it\u2019s possible to implement more granular permissions so that some applications can be assigned more restrictive permissions?\nThis can be achieved using IAM roles for tasks, and splitting the containers according to the permissions required to different task definition profiles\nThis can be achieved by configuring a resource-based policy for each application\nThis cannot be changed as IAM roles can only be linked to container instances\nThis can only be achieved using the Fargate launch type\n",
        "answer": [
            1
        ],
        "explanation": "With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Using this feature you can achieve the required outcome by using IAM roles for tasks and splitting the containers according to the permissions required to different task profiles.\nThe solution can be achieved whether using the EC2 or Fargate launch types\nAmazon ECS does not support IAM resource-based policies\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ecs/\nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/task-iam-roles.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html"
    },
    {
        "question": "44. Question\nYou are putting together the design for a new retail website for a high-profile company. The company has previously been the victim of targeted distributed denial-of-service (DDoS) attacks and have requested that you ensure the design includes mitigation techniques.\nWhich of the following are the BEST techniques to help ensure the availability of the services is not compromised in an attack? (choose 2)\nUse CloudFront for distributing both static and dynamic content\nUse Spot instances to reduce the cost impact in case of attack\nConfigure Auto Scaling with a high maximum number of instances to ensure it can scale accordingly\nUse encryption on your EBS volumes\nUse Placement Groups to ensure high bandwidth and low latency\n",
        "answer": [
            1,
            3
        ],
        "explanation": "CloudFront distributes traffic across multiple edge locations and filters requests to ensure that only valid HTTP(S) requests will be forwarded to backend hosts. CloudFront also supports geoblocking, which you can use to prevent requests from particular geographic locations from being served\nELB automatically distributes incoming application traffic across multiple targets, such as Amazon Elastic Compute Cloud (Amazon EC2) instances, containers, and IP addresses, and multiple Availability Zones, which minimizes the risk of overloading a single resource\nELB, like CloudFront, only supports valid TCP requests, so DDoS attacks such as UDP and SYN floods are not able to reach EC2 instances\nELB also offers a single point of management and can serve as a line of defense between the internet and your backend, private EC2 instances\nAuto Scaling helps to maintain a desired count of EC2 instances running at all times and setting a high maximum number of instances allows your fleet to grow and absorb some of the impact of the attack\nRDS supports several scenarios for deploying DB instances in private and public facing configurations\nCloudWatch can be used to setup alerts for when metrics reach unusual levels. High network in traffic may indicate a DDoS attack\nEncrypting EBS volumes does not help in a DDoS attack as the attack is targeted at reducing availability rather than compromising data\nSpot instances may reduce the cost (depending on the current Spot price) however the questions asks us to focus on availability not cost\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-cloudfront/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-rds/\nhttps://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/\nhttps://docs.aws.amazon.com/waf/latest/developerguide/tutorials-ddos-cross-service.html\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.Scenarios.html"
    },
    {
        "question": "45. Question\nA Solutions Architect is creating the business process workflows associated with an order fulfilment system. What AWS service can assist with coordinating tasks across distributed application components?\nAmazon SNS\nAmazon SWF\nAmazon SQS\nAmazon STS\n",
        "answer": [
            2
        ],
        "explanation": "Amazon Simple Workflow Service (SWF) is a web service that makes it easy to coordinate work across distributed application components. SWF enables applications for a range of use cases, including media processing, web application back-ends, business process workflows, and analytics pipelines, to be designed as a coordination of tasks\nAmazon Security Token Service (STS) is used for requesting temporary credentials\nAmazon Simple Queue Service (SQS) is a message queue used for decoupling application components\nAmazon Simple Notification Service (SNS) is a web service that makes it easy to set up, operate, and send notifications from the cloud\nSNS supports notifications over multiple transports including HTTP/HTTPS, Email/Email-JSON, SQS and SMS\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-swf/"
    },
    {
        "question": "46. Question\nA Solutions Architect has setup a VPC with a public subnet and a VPN-only subnet. The public subnet is associated with a custom route table that has a route to an Internet Gateway. The VPN-only subnet is associated with the main route table and has a route to a virtual private gateway.\nThe Architect has created a new subnet in the VPC and launched an EC2 instance in it. However, the instance cannot connect to the Internet. What is the MOST likely reason?\nThe subnet has been automatically associated with the main route table which does not have a route to the Internet\nThe new subnet has not been associated with a route table\nThe Internet Gateway is experiencing connectivity problems\nThere is no NAT Gateway available in the new subnet so Internet connectivity is not possible\n",
        "answer": [
            1
        ],
        "explanation": "When you create a new subnet, it is automatically associated with the main route table. Therefore, the EC2 instance will not have a route to the Internet. The Architect should associate the new subnet with the custom route table\nNAT Gateways are used for connecting EC2 instances in private subnets to the Internet. This is a valid reason for a private subnet to not have connectivity, however in this case the Architect is attempting to use an Internet Gateway\nSubnets are always associated to a route table when created\nInternet Gateways are highly-available so it\u2019s unlikely that IGW connectivity is the issue\nReferences:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html"
    },
    {
        "question": "47. Question\nYou need to run a production batch process quickly that will use several EC2 instances. The process cannot be interrupted and must be completed within a short time period.\nWhat is likely to be the MOST cost-effective choice of EC2 instance type to use for this requirement?\nFlexible instances\nSpot instances\nReserved instances\nOn-demand instances\n",
        "answer": [
            4
        ],
        "explanation": "The key requirements here are that you need to deploy several EC2 instances quickly to run the batch process and you must ensure that the job completes. The on-demand pricing model is the best for this ad-hoc requirement as though spot pricing may be cheaper you cannot afford to risk that the instances are terminated by AWS when the market price increases\nSpot instances provide a very low hourly compute cost and are good when you have flexible start and end times. They are often used for use cases such as grid computing and high-performance computing (HPC)\nReserved instances are used for longer more stable requirements where you can get a discount for a fixed 1 or 3 year term. This pricing model is not good for temporary requirements\nThere is no such thing as a \u201cflexible instance\u201d\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ec2/"
    },
    {
        "question": "48. Question\nAn Amazon CloudWatch alarm recently notified you that the load on a DynamoDB table you are running is getting close to the provisioned capacity for writes. The DynamoDB table is part of a two-tier customer-facing application and is configured using provisioned capacity. You are concerned about what will happen if the limit is reached but need to wait for approval to increase the WriteCapacityUnits value assigned to the table.\nWhat will happen if the limit for the provisioned capacity for writes is reached?\nThe requests will succeed, and an HTTP 200 status code will be returned\nThe requests will be throttled, and fail with an HTTP 503 code (Service Unavailable)\nThe requests will be throttled, and fail with an HTTP 400 code (Bad Request) and a ProvisionedThroughputExceededException\nDynamoDB scales automatically so there\u2019s no need to worry\n",
        "answer": [
            3
        ],
        "explanation": "DynamoDB can throttle requests that exceed the provisioned throughput for a table. When a request is throttled it fails with an HTTP 400 code (Bad Request) and a ProvisionedThroughputExceeded exception (not a 503 or 200 status code)\nWhen using the provisioned capacity pricing model DynamoDB does not automatically scale. DynamoDB can automatically scale when using the new on-demand capacity mode, however this is not configured for this database\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/"
    },
    {
        "question": "49. Question\nA new application you are deploying uses Docker containers. You are creating a design for an ECS cluster to host the application. Which statements about ECS clusters are correct? (choose 2)\nECS Clusters are a logical grouping of container instances that you can place tasks on\nEach container instance may be part of multiple clusters at a time\nClusters can contain tasks using the Fargate and EC2 launch type\nClusters can contain a single container instance type\nClusters are AZ specific\n",
        "answer": [
            1,
            3
        ],
        "explanation": "ECS Clusters are a logical grouping of container instances the you can place tasks on\nClusters can contain tasks using BOTH the Fargate and EC2 launch type\nEach container instance may only be part of one cluster at a time\nClusters are region specific\nFor clusters with the EC2 launch type clusters can contain different container instance types\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ecs/"
    },
    {
        "question": "50. Question\nA Solutions Architect is designing a three-tier web application that includes an Auto Scaling group of Amazon EC2 Instances running behind an ELB Classic Load Balancer. The security team requires that all web servers must be accessible only through the Elastic Load Balancer and that none of the web servers are directly accessible from the Internet. How should the Architect meet these requirements?\nInstall a Load Balancer on an Amazon EC2 instance\nConfigure the web servers' security group to deny traffic from the Internet\nCreate an Amazon CloudFront distribution in front of the Elastic Load Balancer\nConfigure the web tier security group to allow only traffic from the Elastic Load Balancer\n",
        "answer": [
            4
        ],
        "explanation": "The web servers must be kept private so they will be not have public IP addresses. The ELB is Internet-facing so it will be publicly accessible via it\u2019s DNS address (and corresponding public IP). To restrict web servers to be accessible only through the ELB you can configure the web tier security group to allow only traffic from the ELB. You would normally do this by adding the ELBs security group to the rule on the web tier security group\nThis scenario is using an ELB Classic Load Balancer and these cannot be installed on EC2 instances (at least not by you, in reality all ELBs are actually running on EC2 instances but these are transparent to the AWS end user)\nYou cannot create deny rules in security groups\nCloudFront distributions are used for caching content to improve performance for users on the Internet. They are not security devices to be used for restricting access to EC2 instances\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/"
    },
    {
        "question": "51. Question\nYou are building a new Elastic Container Service (ECS) cluster. The ECS instances are running the EC2 launch type and you would like to enable load balancing to distributed connections to the tasks running on the cluster. You would like the mapping of ports to be performed dynamically and will need to route to different groups of servers based on the path in the requested URL. Which AWS service would you choose to fulfil these requirements?\nClassic Load Balancer\nECS Services\nNetwork Load Balancer\nApplication Load Balancer\n",
        "answer": [
            4
        ],
        "explanation": "An ALB allows containers to use dynamic host port mapping so that multiple tasks from the same service are allowed on the same container host \u2013 the CLB and NLB do not offer this\nAn ALB can also route requests based on the content of the request in the host field: host-based or path-based\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/tutorial-load-balancer-routing.html"
    },
    {
        "question": "52. Question\nYou have created a VPC with private and public subnets and will be deploying a new mySQL database server running on an EC2 instance. According to AWS best practice, which subnet should you deploy the database server into?\nThe private subnet\nThe public subnet\nIt doesn\u2019t matter\nThe subnet that is mapped to the primary AZ in the region\n",
        "answer": [
            1
        ],
        "explanation": "AWS best practice is to deploy databases into private subnets wherever possible. You can then deploy your web front-ends into public subnets and configure these, or an additional application tier to write data to the database\nPublic subnets are typically used for web front-ends as they are directly accessible from the Internet. It is preferable to launch your database in a private subnet\nThere is no such thing as a \u201cprimary\u201d Availability Zone (AZ). All AZs are essentially created equal and your subnets map 1:1 to a single AZ\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/"
    },
    {
        "question": "53. Question\nYou just attempted to restart a stopped EC2 instance and it immediately changed from a pending state to a terminated state. What are the most likely explanations? (choose 2)\nAWS does not currently have enough available On-Demand capacity to service your request\nYou've reached your EBS volume limit\nThe AMI is unsupported\nYou have reached the limit on the number of instances that you can launch in a region\nAn EBS snapshot is corrupt\n",
        "answer": [
            2,
            5
        ],
        "explanation": "The following are a few reasons why an instance might immediately terminate:\n\u2013         You\u2019ve reached your EBS volume limit\n\u2013         An EBS snapshot is corrupt\n\u2013         The root EBS volume is encrypted and you do not have permissions to access the KMS key for decryption\n\u2013         The instance store-backed AMI that you used to launch the instance is missing a required part (an image.part.xx file)\nIt is possible that an instance type is not supported by an AMI and this can cause an \u201cUnsupportedOperation\u201d client error. However, in this case the instance was previously running (it is in a stopped state) so it is unlikely that this is the issue\nIf AWS does not have capacity available a InsufficientInstanceCapacity error will be generated when you try to launch a new instance or restart a stopped instance\nIf you\u2019ve reached the limit on the number of instances you can launch in a region you get an InstanceLimitExceeded error when you try to launch a new instance or restart a stopped instance\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html"
    },
    {
        "question": "54. Question\nYour organization has a data lake on S3 and you need to find a solution for performing in-place queries of the data assets in the data lake. The requirement is to perform both data discovery and SQL querying, and complex queries from a large number of concurrent users using BI tools.\nWhat is the BEST combination of AWS services to use in this situation? (choose 2)\nAWS Lambda for the complex queries\nAmazon Athena for the ad hoc SQL querying\nRedShift Spectrum for the complex queries\nAWS Glue for the ad hoc SQL querying\n",
        "answer": [
            2,
            3
        ],
        "explanation": "Performing in-place queries on a data lake allows you to run sophisticated analytics queries directly on the data in S3 without having to load it into a data warehouse\nYou can use both Athena and Redshift Spectrum against the same data assets. You would typically use Athena for ad hoc data discovery and SQL querying, and then use Redshift Spectrum for more complex queries and scenarios where a large number of data lake users want to run concurrent BI and reporting workloads\nAWS Lambda is a serverless technology for running functions, it is not the best solution for running analytics queries\nAWS Glue is an ETL service\nReferences:\nhttps://docs.aws.amazon.com/aws-technical-content/latest/building-data-lakes/in-place-querying.html\nhttps://aws.amazon.com/redshift/\nhttps://aws.amazon.com/athena/"
    },
    {
        "question": "55. Question\nYou have been asked to come up with a solution for providing single sign-on to existing staff in your company who manage on-premise web applications and now need access to the AWS management console to manage resources in the AWS cloud.\nWhich product combinations provide the best solution to achieve this requirement?\nUse IAM and Amazon Cognito\nUse your on-premise LDAP directory with IAM\nUse IAM and MFA\nUse the AWS Secure Token Service (STS) and SAML\n",
        "answer": [
            4
        ],
        "explanation": "Single sign-on using federation allows users to login to the AWS console without assigning IAM credentials\nThe AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for IAM users or for users that you authenticate (such as federated users from an on-premise directory)\nFederation (typically Active Directory) uses SAML 2.0 for authentication and grants temporary access based on the users AD credentials. The user does not need to be a user in IAM\nYou cannot use your on-premise LDAP directory with IAM, you must use federation\nEnabling multi-factor authentication (MFA) for IAM is not a federation solution\nAmazon Cognito is used for authenticating users to web and mobile apps not for providing single sign-on between on-premises directories and the AWS management console\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/"
    },
    {
        "question": "56. Question\nAn EC2 instance in an Auto Scaling Group is having some issues that are causing the ASG to launch new instances based on the dynamic scaling policy. You need to troubleshoot the EC2 instance and prevent the ASG from launching new instances temporarily.\nWhat is the best method to accomplish this? (choose 2)\nRemove the EC2 instance from the Target Group\nDisable the dynamic scaling policy\nPlace the EC2 instance that is experiencing issues into the Standby state\nDisable the launch configuration associated with the EC2 instance\nSuspend the scaling processes responsible for launching new instances\n",
        "answer": [
            3,
            5
        ],
        "explanation": "You can suspend and then resume one or more of the scaling processes for your Auto Scaling group. This can be useful when you want to investigate a configuration problem or other issue with your web application and then make changes to your application, without invoking the scaling processes. You can manually move an instance from an ASG and put it in the standby state\nInstances in standby state are still managed by Auto Scaling, are charged as normal, and do not count towards available EC2 instance for workload/application use. Auto scaling does not perform health checks on instances in the standby state. Standby state can be used for performing updates/changes/troubleshooting etc. without health checks being performed or replacement instances being launched\nYou do not need to disable the dynamic scaling policy, you can just suspend it as previously described\nYou cannot disable the launch configuration and you can\u2019t modify a launch configuration after you\u2019ve created it\nTarget Groups are features of ELB (specifically ALB/NLB). Removing the instance from the target group will stop the ELB from sending connections to it but will not stop Auto Scaling from launching new instances while you are troubleshooting it\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/"
    },
    {
        "question": "57. Question\nTo increase the resiliency of your RDS DB instance, you decided to enable Multi-AZ. Where will the new standby RDS instance be created?\nYou must specify the location when configuring Multi-AZ\nIn another subnet within the same AZ\nIn the same AWS Region but in a different AZ for high availability\nIn a different AWS Region to protect against Region failures\n",
        "answer": [
            3
        ],
        "explanation": "Multi-AZ RDS creates a replica in another AZ within the same region and synchronously replicates to it (DR only). You cannot choose which AZ in the region will be chosen to create the standby DB instance\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-rds/"
    },
    {
        "question": "58. Question\nYour Systems Administrators currently use Chef for configuration management of on-premise servers. Which AWS service will provide a fully-managed configuration management service that will allow you to use your existing Chef cookbooks?\nElastic Beanstalk\nOpsWorks for Chef Automate\nOpsworks Stacks\nCloudFormation\n",
        "answer": [
            2
        ],
        "explanation": "AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. AWS OpsWorks for Chef Automate is a fully-managed configuration management service that hosts Chef Automate, a suite of automation tools from Chef for configuration management, compliance and security, and continuous deployment. OpsWorks for Chef Automate is completely compatible with tooling and cookbooks from the Chef community and automatically registers new nodes with your Chef server\nAWS OpsWorks Stacks lets you manage applications and servers on AWS and on-premises and uses Chef Solo. The question does not require the managed solution on AWS to manage on-premises resources, just to use existing cookbooks so this is not the preferred solution\nElastic Beanstalk and CloudFormation are not able to build infrastructure using Chef cookbooks\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-opsworks/"
    },
    {
        "question": "59. Question\nYou would like to store a backup of an Amazon EBS volume on Amazon S3. What is the easiest way of achieving this?\nUse SWF to automatically create a backup of your EBS volumes and then upload them to an S3 bucket\nWrite a custom script to automatically copy your data to an S3 bucket\nCreate a snapshot of the volume\nYou don\u2019t need to do anything, EBS volumes are automatically backed up by default\n",
        "answer": [
            3
        ],
        "explanation": "Snapshots capture a point-in-time state of an instance. Snapshots of Amazon EBS volumes are stored on S3 by design so you only need to take a snapshot and it will automatically be stored on Amazon S3\nEBS volumes are not automatically backed up using snapshots. You need to manually take a snapshot or you can use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation, retention, and deletion of snapshots\nThis is not a good use case for Amazon SWF\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/"
    },
    {
        "question": "60. Question\nWhen using throttling controls with API Gateway what happens when request submissions exceed the steady-state request rate and burst limits?\nThe requests will be buffered in a cache until the load reduces\nAPI Gateway fails the limit-exceeding requests and returns \u201c429 Too Many Requests\u201d error responses to the client\nAPI Gateway drops the requests and does not return a response to the client\nAPI Gateway fails the limit-exceeding requests and returns \u201c500 Internal Server Error\u201d error responses to the client\n",
        "answer": [
            2
        ],
        "explanation": "You can throttle and monitor requests to protect your backend. Resiliency through throttling rules based on the number of requests per second for each HTTP method (GET, PUT). Throttling can be configured at multiple levels including Global and Service Call\nWhen request submissions exceed the steady-state request rate and burst limits, API Gateway fails the limit-exceeding requests and returns 429 Too Many Requests error responses to the client\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-api-gateway/\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html"
    },
    {
        "question": "61. Question\nHealth related data in Amazon S3 needs to be frequently accessed for up to 90 days. After that time the data must be retained for compliance reasons for seven years and is rarely accessed.\nWhich storage classes should be used?\nStore data in STANDARD for 90 days then transition the data to DEEP_ARCHIVE\nStore data in INTELLIGENT_TIERING for 90 days then transition to STANDARD_IA\nStore data in STANDARD for 90 days then expire the data\nStore data in STANDARD for 90 days then transition to REDUCED_REDUNDANCY\n",
        "answer": [
            1
        ],
        "explanation": "In this case the data is frequently accessed so must be stored in standard for the first 90 days. After that the data is still to be kept for compliance reasons but is rarely accessed so is a good use case for DEEP_ARCHIVE\nYou cannot transition from INTELLIGENT_TIERING to STANDARD_IA\nYou cannot transition from any storage class to REDUCED_REDUNDANCY\nExpiring the data is not possible as it must be retained for compliance\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "62. Question\nA manual script that runs a few times a week and completes within 10 minutes needs to be replaced with an automated solution. Which of the following options should an Architect use?\nUse AWS CloudFormation\nUse AWS Lambda\nUse a cron job on an Amazon EC2 instance\nUse AWS Batch\n",
        "answer": [
            2
        ],
        "explanation": "AWS Lambda has a maximum execution time of 900 seconds (15 minutes). Therefore the script will complete within this time. AWS Lambda is the best solution as you don\u2019t need to run any instances (it\u2019s serverless) and therefore you will pay only for the execution time.\nAWS Batch is used for running large numbers of batch computing jobs on AWS. AWS Batch dynamically provisions the EC2 instances. This is not a good solution for an ad-hoc use case such as this one where you just need to run a single script a few times a week.\nCron Jobs are used for scheduling tasks to run on Linux instances. They are used for automating maintenance and administration. This is a workable solution for running a script but does require the instance to be running all the time. Also, AWS prefer you to use services such as AWS Lambda for centralized control and administration.\nAWS CloudFormation is used for launching infrastructure. You can use scripts with AWS CloudFormation but its more about running scripts related to infrastructure provisioning.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-lambda/"
    },
    {
        "question": "63. Question\nA high-performance file system is required for a financial modelling application. The data set will be stored on Amazon S3 and the storage solution must have seamless integration so objects can be accessed as files.\nWhich storage solution should be used?\nAmazon FSx for Windows File Server\nAmazon FSx for Lustre\nAmazon Elastic File System (EFS)\nAmazon Elastic Block Store (EBS)\n",
        "answer": [
            2
        ],
        "explanation": "Amazon FSx for Lustre provides a high-performance file system optimized for fast processing of workloads such as machine learning, high performance computing (HPC), video processing, financial modeling, and electronic design automation (EDA). Amazon FSx works natively with Amazon S3, letting you transparently access your S3 objects as files on Amazon FSx to run analyses for hours to months.\nAmazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system so you can easily move your Windows-based applications that require shared file storage to AWS. This solution integrates with Windows file shares, not with Amazon S3.\nEFS and EBS are not good use cases for this solution. Neither storage solution is capable of presenting Amazon S3 objects as files to the application.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-fsx/\nhttps://aws.amazon.com/fsx/"
    },
    {
        "question": "64. Question\nAmazon EC2 instances in a development environment run between 9am and 5pm Monday-Friday. Production instances run 24/7. Which pricing models should be used? (choose 2)\nUse Spot instances for the development environment\nUse scheduled reserved instances for the development environment\nUse Reserved instances for the production environment\nUse Reserved instances for the development environment\nUse On-Demand instances for the production environment\n",
        "answer": [
            2,
            3
        ],
        "explanation": "Scheduled Instances are a good choice for workloads that do not run continuously but do run on a regular schedule. This is ideal for the development environment\nReserved instances are a good choice for workloads that run continuously. This is a good option for the production environment\nSpot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted. Spot instances are not suitable for the development environment as important work may be interrupted.\nThere is no long-term commitment required when you purchase On-Demand Instances. However, you do not get any discount and therefore this is the most expensive option.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/instance-purchasing-options.html\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ec2/"
    },
    {
        "question": "65. Question\nAn High Performance Computing (HPC) application needs storage that can provide 135,000 IOPS. The storage layer is replicated across all instances in a cluster.\nWhat is the optimal storage solution that provides the required performance and is cost-effective?\nUse Amazon EC2 Enhanced Networking with an EBS HDD Throughput Optimized volume\nUse Amazon S3 with byte-range fetch\nUse Amazon Instance Store\nUse Amazon EBS Provisioned IOPS volume with 135,000 IOPS\n",
        "answer": [
            3
        ],
        "explanation": "Instance stores offer very high performance and low latency. As long as you can afford to lose an instance, i.e. you are replicating your data, these can be a good solution for high performance/low latency requirements. Also, the cost of instance stores is included in the instance charges so it can also be more cost-effective than EBS Provisioned IOPS.\nIn the case of a HPC cluster that replicates data between nodes you don\u2019t necessarily need a shared storage solution such as Amazon EBS Provisioned IOPS \u2013 this would also be a more expensive solution as the Instance Store is included in the cost of the HPC instance.\nAmazon S3 is not a solution for this HPC application as in this case it will require block-based storage to provide the required IOPS.\nEnhanced networking provides higher bandwidth and lower latency and is implemented using an Elastic Network Adapter (ENA). However, using an ENA with an HDD Throughput Optimized volume is not recommended and the volume will not provide the performance required for this use case.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ec2/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html"
    }
]