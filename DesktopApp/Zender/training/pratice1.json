[
    {
        "question": "1. Question\nA company has deployed Amazon RedShift for performing analytics on user data. When using Amazon RedShift, which of the following statements are correct in relation to availability and durability? (choose 2)\nRedShift always keeps three copies of your data\nSingle-node clusters support data replication\nRedShift provides continuous/incremental backups\nManual backups are automatically deleted when you delete a cluster\nRedShift always keeps five copies of your data\n",
        "answer": [
            1,
            3
        ],
        "explanation": "RedShift always keeps three copies of your data and provides continuous/incremental backups\nCorrections:\nSingle-node clusters do not support data replication\nManual backups are not automatically deleted when you delete a cluster\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-redshift/"
    },
    {
        "question": "2. Question\nYou are a Solutions Architect at Digital Cloud Training. A client from a large multinational corporation is working on a deployment of a significant amount of resources into AWS. The client would like to be able to deploy resources across multiple AWS accounts and regions using a single toolset and template. You have been asked to suggest a toolset that can provide this functionality?\nUse a CloudFormation template that creates a stack and specify the logical IDs of each account and region\nUse a CloudFormation StackSet and specify the target accounts and regions in which the stacks will be created\nUse a third-party product such as Terraform that has support for multiple AWS accounts and regions\nThis cannot be done, use separate CloudFormation templates per AWS account and region\n",
        "answer": [
            2
        ],
        "explanation": "AWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation\nUsing an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions. An administrator account is the AWS account in which you create stack sets\nA stack set is managed by signing in to the AWS administrator account in which it was created. A target account is the account into which you create, update, or delete one or more stacks in your stack set\nBefore you can use a stack set to create stacks in a target account, you must set up a trust relationship between the administrator and target accounts\nA regular CloudFormation template cannot be used across regions and accounts. You would need to create copies of the template and then manage updates\nYou do not need to use a third-party product such as Terraform as this functionality can be delivered through native AWS technology\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-concepts.html"
    },
    {
        "question": "3. Question\nA new Big Data application you are developing will use hundreds of EC2 instances to write data to a shared file system. The file system must be stored redundantly across multiple AZs within a region and allow the EC2 instances to concurrently access the file system. The required throughput is multiple GB per second.\nFrom the options presented which storage solution can deliver these requirements?\nAmazon EBS using multiple volumes in a RAID 0 configuration\nAmazon EFS\nAmazon S3\nAmazon Storage Gateway\n",
        "answer": [
            2
        ],
        "explanation": "Amazon EFS is the best solution as it is the only solution that is a file-level storage solution (not block/object-based), stores data redundantly across multiple AZs within a region and you can concurrently connect up to thousands of EC2 instances to a single filesystem\nAmazon EBS volumes cannot be accessed by concurrently by multiple instances\nAmazon S3 is an object store, not a file system\nAmazon Storage Gateway is a range of products used for on-premises storage management and can be configured to cache data locally, backup data to the cloud and also provides a virtual tape backup solution\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-efs/"
    },
    {
        "question": "4. Question\nAn application running on an external website is attempting to initiate a request to your company\u2019s website on AWS using API calls. A problem has been reported in which the requests are failing with an error that includes the following text:\n\u201cCross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource\u201d\nYou have been asked to resolve the problem, what is the most likely solution?\nEnable CORS on the APIs resources using the selected methods under the API Gateway\nThe ACL on the API needs to be updated\nThe IAM policy does not allow access to the API\nThe request is not secured with SSL/TLS\n",
        "answer": [
            1
        ],
        "explanation": "Can enable Cross Origin Resource Sharing (CORS) for multiple domain use with Javascript/AJAX:\n\u2013         Can be used to enable requests from domains other the APIs domain\n\u2013         Allows the sharing of resources between different domains\n\u2013         The method (GET, PUT, POST etc.) for which you will enable CORS must be available in the API Gateway API before you enable CORS\n\u2013         If CORS is not enabled and an API resource received requests from another domain the request will be blocked\n\u2013         Enable CORS on the APIs resources using the selected methods under the API Gateway\nIAM policies are not used to control CORS and there is no ACL on the API to update\nThis error would display whether using SSL/TLS or not\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-api-gateway/"
    },
    {
        "question": "5. Question\nYou need to configure an application to retain information about each user session and have decided to implement a layer within the application architecture to store this information.\nWhich of the options below could be used? (choose 2)\nA workflow service such as Amazon Simple Workflow Service (SWF)\nSticky sessions on an Elastic Load Balancer (ELB)\nA block storage service such as Elastic Block Store (EBS)\nA relational data store such as Amazon RDS\nA key/value store such as ElastiCache Redis\n",
        "answer": [
            2,
            5
        ],
        "explanation": "In order to address scalability and to provide a shared data storage for sessions that can be accessible from any individual web server, you can abstract the HTTP sessions from the web servers themselves. A common solution to for this is to leverage an In-Memory Key/Value store such as Redis and Memcached.\nSticky sessions, also known as session affinity, allow you to route a site user to the particular web server that is managing that individual user\u2019s session. The session\u2019s validity can be determined by a number of methods, including a client-side cookie or via configurable duration parameters that can be set at the load balancer which routes requests to the web servers. You can configure sticky sessions on Amazon ELBs.\nRelational databases are not typically used for storing session state data due to their rigid schema that tightly controls the format in which data can be stored.\nWorkflow services such as SWF are used for carrying out a series of tasks in a coordinated task flow. They are not suitable for storing session state data.\nIn this instance the question states that a caching layer is being implemented and EBS volumes would not be suitable for creating an independent caching layer as they must be attached to EC2 instances.\nReferences:\nhttps://aws.amazon.com/caching/session-management/"
    },
    {
        "question": "6. Question\nThe data scientists in your company are looking for a service that can process and analyze real-time, streaming data. They would like to use standard SQL queries to query the streaming data.\nWhich combination of AWS services would deliver these requirements?\nKinesis Data Streams and Kinesis Data Analytics\nKinesis Data Streams and Kinesis Firehose\nElastiCache and EMR\nDynamoDB and EMR\n",
        "answer": [
            1
        ],
        "explanation": "Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs\nAmazon Kinesis Data Analytics is the easiest way to process and analyze real-time, streaming data. Kinesis Data Analytics can use standard SQL queries to process Kinesis data streams and can ingest data from Kinesis Streams and Kinesis Firehose but Firehose cannot be used for running SQL queries\nDynamoDB is a NoSQL database that can be used for storing data from a stream but cannot be used to process or analyze the data or to query it with SQL queries. Elastic Map Reduce (EMR) is a hosted Hadoop framework and is not used for analytics on streaming data\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-kinesis/"
    },
    {
        "question": "7. Question\nWhich of the following approaches provides the lowest cost for Amazon elastic block store snapshots while giving you the ability to fully restore data?\nMaintain a single snapshot; the latest snapshot is both incremental and complete\nMaintain the most current snapshot; archive the original to Amazon Glacier\nMaintain two snapshots: the original snapshot and the latest incremental snapshot\nMaintain the original snapshot; subsequent snapshots will overwrite one another\n",
        "answer": [
            1
        ],
        "explanation": "You can backup data on an EBS volume by periodically taking snapshots of the volume. The scenario is that you need to reduce storage costs by maintaining as few EBS snapshots as possible whilst ensuring you can restore all data when required.\nIf you take periodic snapshots of a volume, the snapshots are incremental which means only the blocks on the device that have changed after your last snapshot are saved in the new snapshot. Even though snapshots are saved incrementally, the snapshot deletion process is designed such that you need to retain only the most recent snapshot in order to restore the volume\nYou cannot just keep the original snapshot as it will not be incremental and complete\nYou do not need to keep the original and latest snapshot as the latest snapshot is all that is needed\nThere is no need to archive the original snapshot to Amazon Glacier. EBS copies your data across multiple servers in an AZ for durability\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/"
    },
    {
        "question": "8. Question\nYou are undertaking a project to make some audio and video files that your company uses for onboarding new staff members available via a mobile application. You are looking for a cost-effective way to convert the files from their current formats into formats that are compatible with smartphones and tablets. The files are currently stored in an S3 bucket.\nWhat AWS service can help with converting the files?\nRekognition\nElastic Transcoder\nData Pipeline\nAmazon Personalize\n",
        "answer": [
            2
        ],
        "explanation": "Amazon Elastic Transcoder is a highly scalable, easy to use and cost-effective way for developers and businesses to convert (or \u201ctranscode\u201d) video and audio files from their source format into versions that will playback on devices like smartphones, tablets and PCs\nAmazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications\nData Pipeline helps you move, integrate, and process data across AWS compute and storage resources, as well as your on-premises resources\nRekognition is a deep learning-based visual analysis service\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/media-services/amazon-elastic-transcoder/"
    },
    {
        "question": "9. Question\nYou are a Solutions Architect at Digital Cloud Training. A large multi-national client has requested a design for a multi-region, multi-master database. The client has requested that the database be designed for fast, massively scaled applications for a global user base. The database should be a fully managed service including the replication.\nWhich AWS service can deliver these requirements?\nS3 with Cross Region Replication\nRDS with Multi-AZ\nDynamoDB with Global Tables and Cross Region Replication\nEC2 instances with EBS replication\n",
        "answer": [
            3
        ],
        "explanation": "Cross-region replication allows you to replicate across regions:\n\u2013         Amazon DynamoDB global tables provides a fully managed solution for deploying a multi-region, multi-master database\n\u2013         When you create a global table, you specify the AWS regions where you want the table to be available\n\u2013         DynamoDB performs all of the necessary tasks to create identical tables in these regions, and propagate ongoing data changes to all of them\nRDS with Multi-AZ is not multi-master (only one DB can be written to at a time), and does not span regions\nS3 is an object store not a multi-master database\nThere is no such thing as EBS replication. You could build your own database stack on EC2 with DB-level replication but that is not what is presented in the answer\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/"
    },
    {
        "question": "10. Question\nA customer has asked you to recommend the best solution for a highly available database. The database is a relational OLTP type of database and the customer does not want to manage the operating system the database runs on. Failover between AZs must be automatic.\nWhich of the below options would you suggest to the customer?\nUse RDS in a Multi-AZ configuration\nUse DynamoDB\nUse RedShift in a Multi-AZ configuration\nInstall a relational database on EC2 instances in multiple AZs and create a cluster\n",
        "answer": [
            1
        ],
        "explanation": "Amazon Relational Database Service (Amazon RDS) is a managed service that makes it easy to set up, operate, and scale a relational database in the cloud. With RDS you can configure Multi-AZ which creates a replica in another AZ and synchronously replicates to it (DR only)\nRedShift is used for analytics OLAP not OLTP\nIf you install a DB on an EC2 instance you will need to manage to OS yourself and the customer wants it to be managed for them\nDynamoDB is a managed database of the NoSQL type. NoSQL DBs are not relational DBs\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/"
    },
    {
        "question": "11. Question\nAn application you manage uses Auto Scaling and a fleet of EC2 instances. You recently noticed that Auto Scaling is scaling the number of instances up and down multiple times in the same hour. You need to implement a remediation to reduce the amount of scaling events. The remediation must be cost-effective and preserve elasticity. What design changes would you implement? (choose 2)\nModify the Auto Scaling group termination policy to terminate the newest instance first\nModify the Auto Scaling group cool-down timers\nModify the CloudWatch alarm period that triggers your Auto Scaling scale down policy\nModify the Auto Scaling policy to use scheduled scaling actions\nModify the Auto Scaling group termination policy to terminate the oldest instance first\n",
        "answer": [
            2,
            3
        ],
        "explanation": "The cooldown period is a configurable setting for your Auto Scaling group that helps to ensure that it doesn\u2019t launch or terminate additional instances before the previous scaling activity takes effect so this would help. After the Auto Scaling group dynamically scales using a simple scaling policy, it waits for the cooldown period to complete before resuming scaling activities\nThe CloudWatch Alarm Evaluation Period is the number of the most recent data points to evaluate when determining alarm state. This would help as you can increase the number of datapoints required to trigger an alarm\nThe order in which Auto Scaling terminates instances is not the issue here, the problem is that the workload is dynamic and Auto Scaling is constantly reacting to change, and launching or terminating instances\nUsing scheduled scaling actions may not be cost-effective and also affects elasticity as it is less dynamic\nReferences:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html#alarm-evaluation\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/"
    },
    {
        "question": "12. Question\nOne of your EC2 instances runs an application process that saves user data to an attached EBS volume. The EBS volume was attached to the EC2 instance after it was launched and is unencrypted. You would like to encrypt the data that is stored on the volume as it is considered sensitive however you cannot shutdown the instance due to other application processes that are running.\nWhat is the best method of applying encryption to the sensitive data without any downtime?\nCreate an encrypted snapshot of the current EBS volume. Restore the snapshot to the EBS volume\nCreate and mount a new encrypted EBS volume. Move the data to the new volume and then delete the old volume\nUnmount the volume and enable server-side encryption. Re-mount the EBS volume\nLeverage the AWS Encryption CLI to encrypt the data on the volume\n",
        "answer": [
            2
        ],
        "explanation": "You cannot restore a snapshot of a root volume without downtime\nThere is no direct way to change the encryption state of a volume\nEither create an encrypted volume and copy data to it or take a snapshot, encrypt it, and create a new encrypted volume from the snapshot\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/"
    },
    {
        "question": "13. Question\nYou are planning to launch a RedShift cluster for processing and analyzing a large amount of data. The RedShift cluster will be deployed into a VPC with multiple subnets. Which construct is used when provisioning the cluster to allow you to specify a set of subnets in the VPC that the cluster will be deployed into?\nSubnet Group\nAvailability Zone (AZ)\nDB Subnet Group\nCluster Subnet Group\n",
        "answer": [
            4
        ],
        "explanation": "You create a cluster subnet group if you are provisioning your cluster in your virtual private cloud (VPC)\nA cluster subnet group allows you to specify a set of subnets in your VPC\nWhen provisioning a cluster, you provide the subnet group and Amazon Redshift creates the cluster on one of the subnets in the group\nA DB Subnet Group is used by RDS\nA Subnet Group is used by ElastiCache\nAvailability Zones are part of the AWS global infrastructure, subnets reside within AZs but in RedShift you provision the cluster into Cluster Subnet Groups\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-redshift/\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-cluster-subnet-groups.html"
    },
    {
        "question": "14. Question\nA Solutions Architect is responsible for a web application that runs on EC2 instances that sit behind an Application Load Balancer (ALB). Auto Scaling is used to launch instances across 3 Availability Zones. The web application serves large image files and these are stored on an Amazon EFS file system. Users have experienced delays in retrieving the files and the Architect has been asked to improve the user experience.\nWhat should the Architect do to improve user experience?\nCache static content using CloudFront\nReduce the file size of the images\nMove the digital assets to EBS\nUse Spot instances\n",
        "answer": [
            1
        ],
        "explanation": "CloudFront is ideal for caching static content such as the files in this scenario and would increase performance\nMoving the files to EBS would not make accessing the files easier or improve performance\nReducing the file size of the images may result in better retrieval times, however CloudFront would still be the preferable option\nUsing Spot EC2 instances may reduce EC2 costs but it won\u2019t improve user experience\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-cloudfront/"
    },
    {
        "question": "15. Question\nA Solutions Architect is deploying an Auto Scaling Group (ASG) and needs to determine what CloudWatch monitoring option to use. Which of the statements below would assist the Architect in making his decision? (choose 2)\nBasic monitoring is enabled by default if the ASG is created from the CLI\nDetailed monitoring is chargeable and must always be manually enabled\nDetailed monitoring is free and can be manually enabled\nDetailed monitoring is enabled by default if the ASG is created from the CLI\nBasic monitoring is enabled by default if the ASG is created from the console\n",
        "answer": [
            4,
            5
        ],
        "explanation": "Basic monitoring sends EC2 metrics to CloudWatch about ASG instances every 5 minutes\nDetailed can be enabled and sends metrics every 1 minute (it is always chargeable)\nWhen the launch configuration is created from the CLI detailed monitoring of EC2 instances is enabled by default\nWhen you enable Auto Scaling group metrics, Auto Scaling sends sampled data to CloudWatch every minute\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/"
    },
    {
        "question": "16. Question\nA Linux instance running in your VPC requires some configuration changes to be implemented locally and you need to run some commands. Which of the following can be used to securely connect to the instance?\nSSL/TLS certificate\nPublic key\nKey Pairs\nEC2 password\n",
        "answer": [
            3
        ],
        "explanation": "A key pair consists of a public key that AWS stores, and a private key file that you store\nFor Windows AMIs, the private key file is required to obtain the password used to log into your instance\nFor Linux AMIs, the private key file allows you to securely SSH into your instance\nThe \u201cEC2 password\u201d might refer to the operating system password. By default, you cannot login this way to Linux and must use a key pair. However, this can be enabled by setting a password and updating the /etc/ssh/sshd_config file\nYou cannot login to an EC2 instance using certificates/public keys\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ec2/"
    },
    {
        "question": "17. Question\nYour company would like to restrict the ability of most users to change their own passwords whilst continuing to allow a select group of users within specific user groups.\nWhat is the best way to achieve this? (choose 2)\nUnder the IAM Password Policy deselect the option to allow users to change their own passwords\nCreate an IAM Policy that grants users the ability to change their own password and attach it to the groups that contain the users\nCreate an IAM Policy that grants users the ability to change their own password and attach it to the individual user accounts\nCreate an IAM Role that grants users the ability to change their own password and attach it to the groups that contain the users\nDisable the ability for all users to change their own passwords using the AWS Security Token Service\n",
        "answer": [
            1,
            2
        ],
        "explanation": "A password policy can be defined for enforcing password length, complexity etc. (applies to all users)\nYou can allow or disallow the ability to change passwords using an IAM policy and you should attach this to the group that contains the users, not to the individual users themselves\nYou cannot use an IAM role to perform this function\nThe AWS STS is not used for controlling password policies\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/"
    },
    {
        "question": "18. Question\nA colleague from your company\u2019s IT Security team has notified you of an Internet-based threat that affects a certain port and protocol combination. You have conducted an audit of your VPC and found that this port and protocol combination is allowed on an Inbound Rule with a source of 0.0.0.0/0. You have verified that this rule only exists for maintenance purposes and need to make an urgent change to block the access.\nWhat is the fastest way to block access from the Internet to the specific ports and protocols?\nYou don\u2019t need to do anything; this rule will only allow access to VPC based resources\nUpdate the security group by removing the rule\nDelete the security group\nAdd a deny rule to the security group with a higher priority\n",
        "answer": [
            2
        ],
        "explanation": "Security group membership can be changed whilst instances are running\nAny changes to security groups will take effect immediately\nYou can only assign permit rules in a security group, you cannot assign deny rules\nIf you delete the security you will remove all rules and potentially cause other problems\nYou do need to make the update, as it\u2019s the VPC based resources you\u2019re concerned about\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/"
    },
    {
        "question": "19. Question\nYou are an entrepreneur building a small company with some resources running on AWS. As you have limited funding, you\u2019re extremely cost conscious. Which AWS service can send you alerts via email or SNS topic when you are forecast to exceed your funding capacity so you can take action?\nCost & Usage reports\nAWS Billing Dashboard\nAWS Budgets\nCost Explorer\n",
        "answer": [
            3
        ],
        "explanation": "AWS Budgets gives you the ability to set custom budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount. Budget alerts can be sent via email and/or Amazon Simple Notification Service (SNS) topic\nThe AWS Cost Explorer is a free tool that allows you to view charts of your costs\nThe AWS Billing Dashboard can send alerts when you\u2019re bill reaches certain thresholds but you must use AWS Budgets to created custom budgets that notify you when you are forecast to exceed a budget\nThe AWS Cost and Usage report tracks your AWS usage and provides estimated charges associated with your AWS account but does not send alerts\nReferences:\nhttps://aws.amazon.com/aws-cost-management/aws-budgets/"
    },
    {
        "question": "20. Question\nYour company is starting to use AWS to host new web-based applications. A new two-tier application will be deployed that provides customers with access to data records. It is important that the application is highly responsive and retrieval times are optimized. You\u2019re looking for a persistent data store that can provide the required performance. From the list below what AWS service would you recommend for this requirement?\nKinesis Data Streams\nElastiCache with the Memcached engine\nElastiCache with the Redis engine\nRDS in a multi-AZ configuration\n",
        "answer": [
            3
        ],
        "explanation": "ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol-compliant server nodes in the cloud. The in-memory caching provided by ElastiCache can be used to significantly improve latency and throughput for many read-heavy application workloads or compute-intensive workloads\nThere are two different database engines with different characteristics as per below:\nMemcached\nNot persistent\nCannot be used as a data store\nSupports large nodes with multiple cores or threads\nScales out and in, by adding and removing nodes\nRedis\nData is persistent\nCan be used as a datastore\nNot multi-threaded\nScales by adding shards, not nodes\nKinesis Data Streams is used for processing streams of data, it is not a persistent data store\nRDS is not the optimum solution due to the requirement to optimize retrieval times which is a better fit for an in-memory data store such as ElastiCache\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-elasticache/"
    },
    {
        "question": "21. Question\nA Solutions Architect is developing an encryption solution. The solution requires that data keys are encrypted using envelope protection before they are written to disk.\nWhich solution option can assist with this requirement?\nAWS Certificate Manager\nAWS KMS API\nIAM Access Key\nAPI Gateway with STS\n",
        "answer": [
            2
        ],
        "explanation": "The AWS KMS API can be used for encrypting data keys (envelope encryption)\nAWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources\nThe AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users)\nIAM access keys are used for signing programmatic requests you make to AWS\nReferences:\nhttps://docs.aws.amazon.com/kms/latest/APIReference/Welcome.html"
    },
    {
        "question": "22. Question\nA solutions Architect is designing a new workload where an AWS Lambda function will access an Amazon DynamoDB table.\nWhat is the MOST secure means of granting the Lambda function access to the DynamoDB table?\nCreate an identity and access management (IAM) role with the necessary permissions to access the DynamoDB table, and assign the role to the Lambda function\nCreate a DynamoDB username and password and give them to the Developer to use in the Lambda function\nCreate an identity and access management (IAM) user and create access and secret keys for the user. Give the user the necessary permissions to access the DynamoDB table. Have the Developer use these keys to access the resources\nCreate an identity and access management (IAM) role allowing access from AWS Lambda and assign the role to the DynamoDB table\n",
        "answer": [
            1
        ],
        "explanation": "The most secure method is to use an IAM role so you don\u2019t need to embed any credentials in code and can tightly control the services that your Lambda function can access. You need to assign the role to the Lambda function, NOT to the DynamoDB table\nYou should not provide a username and password to the Developer to use with the function. This is insecure \u2013 always avoid using credentials in code!\nYou should not use an access key and secret ID to access DynamoDB. Again, this means embedding credentials in code which should be avoided.\nReferences:\nhttps://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/"
    },
    {
        "question": "23. Question\nAn e-commerce application is hosted in AWS. The last time a new product was launched, the application experienced a performance issue due to an enormous spike in traffic. Management decided that capacity must be doubled this week after the product is launched.\nWhat is the MOST efficient way for management to ensure that capacity requirements are met?\nAdd Amazon EC2 Spot instances\nAdd a Step Scaling policy\nAdd a Simple Scaling policy\nAdd a Scheduled Scaling action\n",
        "answer": [
            4
        ],
        "explanation": "Scheduled scaling: Scaling based on a schedule allows you to set your own scaling schedule for predictable load changes. To configure your Auto Scaling group to scale based on a schedule, you create a scheduled action. This is ideal for situations where you know when and for how long you are going to need the additional capacity\nStep scaling: step scaling policies increase or decrease the current capacity of your Auto Scaling group based on a set of scaling adjustments, known as step adjustments. The adjustments vary based on the size of the alarm breach. This is more suitable to situations where the load unpredictable\nSimple scaling: AWS recommend using step over simple scaling in most cases. With simple scaling, after a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms (in contrast to step scaling). Again, this is more suitable to unpredictable workloads\nEC2 Spot Instances: adding spot instances may decrease EC2 costs but you still need to ensure they are available. The main requirement of the question is that the performance issues are resolved rather than the cost being minimized\nReferences:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/"
    },
    {
        "question": "24. Question\nA colleague has asked you some questions about how AWS charge for DynamoDB. He is interested in knowing what type of workload DynamoDB is best suited for in relation to cost and how AWS charges for DynamoDB? (choose 2)\nDynamoDB is more cost effective for read heavy workloads\nDynamoDB is more cost effective for write heavy workloads\nPriced based on provisioned throughput (read/write) regardless of whether you use it or not\nYou provision for expected throughput but are only charged for what you use\nDynamoDB scales vertically by adding additional nodes\n",
        "answer": [
            1,
            3
        ],
        "explanation": "DynamoDB is more cost effective for read heavy workloads. This is due to the read capacity units (RCU) being half the price of the write capacity units (WCU).\nWith DynamoDB you are charged based on the provisioned throughput you assign (RCUs/WCUs) regardless of whether you use it or not. With the DynamoDB Auto Scaling feature you can now have DynamoDB dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. However, this is not provided as an answer option.\nDynamoDB scales horizontally and the mechanism by which this happens is transparent to consumers. It does not scale vertically by adding nodes.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html"
    },
    {
        "question": "25. Question\nA user is testing a new service that receives location updates from 5,000 rental cars every hour. Which service will collect data and automatically scale to accommodate production workload?\nAmazon API Gateway\nAmazon EBS\nAmazon Kinesis Firehose\nAmazon EC2\n",
        "answer": [
            3
        ],
        "explanation": "What we need here is a service that can streaming collect streaming data. The only option available is Kinesis Firehose which captures, transforms, and loads streaming data into \u201cdestinations\u201d such as S3, RedShift, Elasticsearch and Splunk\nAmazon EC2 is not suitable for collecting streaming data\nEBS is a block-storage service in which you attach volumes to EC2 instances, this does not assist with collecting streaming data (see previous point)\nAmazon API Gateway is used for hosting and managing APIs not for receiving streaming data\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-kinesis/"
    },
    {
        "question": "26. Question\nA research company is developing a data lake solution in Amazon S3 to analyze huge datasets. The solution makes infrequent SQL queries only. In addition, the company wants to minimize infrastructure costs.\nWhich AWS service should be used to meet these requirements?\nAmazon Redshift Spectrum\nAmazon Aurora\nAmazon Athena\nAmazon RDS for MySQL\n",
        "answer": [
            3
        ],
        "explanation": "Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run \u2013 this satisfies the requirement to minimize infrastructure costs for infrequent queries.\nAmazon RedShift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required. However, RedShift nodes run on EC2 instances, so for infrequent queries this will not minimize infrastructure costs.\nAmazon RDS and Aurora are not suitable solutions for analyzing datasets on S3 \u2013 these are both relational databases typically used for transactional (not analytical) workloads.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-athena/\nhttps://docs.aws.amazon.com/athena/latest/ug/what-is.html"
    },
    {
        "question": "27. Question\nAn application runs on two EC2 instances in private subnets split between two AZs. The application needs to connect to a CRM SaaS application running on the Internet. The vendor of the SaaS application restricts authentication to a whitelist of source IP addresses and only 2 IP addresses can be configured per customer.\nWhat is the most appropriate and cost-effective solution to enable authentication to the SaaS application?\nConfigure redundant Internet Gateways and update the routing tables for each subnet\nConfigure a NAT Gateway for each AZ with an Elastic IP address\nUse multiple Internet-facing Application Load Balancers with Elastic IP addresses\nUse a Network Load Balancer and configure a static IP for each AZ\n",
        "answer": [
            2
        ],
        "explanation": "In this scenario you need to connect the EC2 instances to the SaaS application with a source address of one of two whitelisted public IP addresses to ensure authentication works.\nA NAT Gateway is created in a specific AZ and can have a single Elastic IP address associated with it. NAT Gateways are deployed in public subnets and the route tables of the private subnets where the EC2 instances reside are configured to forward Internet-bound traffic to the NAT Gateway. You do pay for using a NAT Gateway based on hourly usage and data processing, however this is still a cost-effective solution\nA Network Load Balancer can be configured with a single static IP address (the other types of ELB cannot) for each AZ. However, using a NLB is not an appropriate solution as the connections are being made outbound from the EC2 instances to the SaaS app and ELBs are used for distributing inbound connection requests to EC2 instances (only return traffic goes back through the ELB)\nAn ALB does not support static IP addresses and is not suitable for a proxy function\nAWS Route 53 is a DNS service and is not used as an outbound proxy server so is not suitable for this scenario\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/"
    },
    {
        "question": "28. Question\nA recent security audit uncovered some poor deployment and configuration practices within your VPC. You need to ensure that applications are deployed in secure configurations.\nHow can this be achieved in the most operationally efficient manner?\nRemove the ability for staff to deploy applications\nUse AWS Inspector to apply secure configurations\nManually check all application configurations before deployment\nUse CloudFormation with securely configured templates\n",
        "answer": [
            4
        ],
        "explanation": "CloudFormation helps users to deploy resources in a consistent and orderly way. By ensuring the CloudFormation templates are created and administered with the right security configurations for your resources, you can then repeatedly deploy resources with secure settings and reduce the risk of human error\nRemoving the ability of staff to deploy resources does not help you to deploy applications securely as it does not solve the problem of how to do this in an operationally efficient manner\nManual checking of all application configurations before deployment is not operationally efficient\nAmazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not used to secure the actual deployment of resources, only to assess the deployed state of the resources\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/"
    },
    {
        "question": "29. Question\nThe application development team in your company has a new requirement for the deployment of a container solution. You plan to use the AWS Elastic Container Service (ECS). The solution should include load balancing of incoming requests across the ECS containers and allow the containers to use dynamic host port mapping so that multiple tasks from the same service can run on the same container host.\nWhich AWS load balancing configuration will support this?\nYou cannot run multiple copies of a task on the same instance, because the ports would conflict\nUse a Network Load Balancer (NLB) and host-based routing\nUse a Classic Load Balancer (CLB) and create a static mapping of the ports\nUse an Application Load Balancer (ALB) and map the ECS service to the ALB\n",
        "answer": [
            4
        ],
        "explanation": "It is possible to associate a service on Amazon ECS to an Application Load Balancer (ALB) for the Elastic Load Balancing (ELB) service\nAn Application Load Balancer allows dynamic port mapping. You can have multiple tasks from a single service on the same container instance.\nThe Classic Load Balancer requires that you statically map port numbers on a container instance. You cannot run multiple copies of a task on the same instance, because the ports would conflict\nAn NLB does not support host-based routing (ALB only), and this would not help anyway\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ecs/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/"
    },
    {
        "question": "30. Question\nTo improve security in your AWS account you have decided to enable multi-factor authentication (MFA). You can authenticate using an MFA device in which two ways? (choose 2)\nUsing biometrics\nLocally to EC2 instances\nThrough the AWS Management Console\nUsing the AWS API\nUsing a key pair\n",
        "answer": [
            3,
            4
        ],
        "explanation": "You can authenticate using an MFA device in the following ways:\nThrough the AWS Management Console \u2013 the user is prompted for a user name, password and authentication code\nUsing the AWS API \u2013 restrictions are added to IAM policies and developers can request temporary security credentials and pass MFA parameters in their AWS STS API requests\nUsing the AWS CLI by obtaining temporary security credentials from STS (aws sts get-session-token)\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/"
    },
    {
        "question": "31. Question\nThe company you work for has a presence across multiple AWS regions. As part of disaster recovery planning you are formulating a solution to provide a regional DR capability for an application running on a fleet of Amazon EC2 instances that are provisioned by an Auto Scaling Group (ASG). The applications are stateless and read and write data to an S3 bucket. You would like to utilize the current AMI used by the ASG as it has some customizations made to it.\nWhat are the steps you might take to enable a regional DR capability for this application? (choose 2)\nEnable cross region replication on the S3 bucket and specify a destination bucket in the DR region\nModify the launch configuration for the ASG in the DR region and specify the AMI\nCopy the AMI to the DR region and create a new launch configuration for the ASG that uses the AMI\nEnable multi-AZ for the S3 bucket to enable synchronous replication to the DR region\nModify the permissions of the AMI so it can be used across multiple regions\n",
        "answer": [
            1,
            3
        ],
        "explanation": "There are two parts to this solution. First you need to copy the S3 data to each region (as the instances are stateless), then you need to be able to deploy instances from an ASG using the same AMI in each regions.\n\u2013         CRR is an Amazon S3 feature that automatically replicates data across AWS Regions. With CRR, every object uploaded to an S3 bucket is automatically replicated to a destination bucket in a different AWS Region that you choose, this enables you to copy the existing data across to each region\n\u2013         AMIs of both Amazon EBS-backed AMIs and instance store-backed AMIs can be copied between regions. You can then use the copied AMI to create a new launch configuration (remember that you cannot modify an ASG launch configuration, you must create a new launch configuration)\nThere\u2019s no such thing as Multi-AZ for an S3 bucket (it\u2019s an RDS concept)\nChanging permissions on an AMI doesn\u2019t make it usable from another region, the AMI needs to be present within each region to be used\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/"
    },
    {
        "question": "32. Question\nA Solutions Architect needs to improve performance for a web application running on EC2 instances launched by an Auto Scaling group. The instances run behind an ELB Application Load Balancer. During heavy use periods the ASG doubles in size and analysis has shown that static content stored on the EC2 instances is being requested by users in a specific geographic location.\nHow can the Solutions Architect reduce the need to scale and improve the application performance?\nRe-deploy the application in a new VPC that is closer to the users making the requests\nCreate an Amazon CloudFront distribution for the site and redirect user traffic to the distribution\nStore the contents on Amazon EFS instead of the EC2 root volume\nImplement Amazon Redshift to create a repository of the content closer to the users\n",
        "answer": [
            2
        ],
        "explanation": "This is a good use case for CloudFront. CloudFront is a content delivery network (CDN) that caches content closer to users. You can cache the static content on CloudFront using the EC2 instances as origins for the content. This will improve performance (as the content is closer to the users) and reduce the need for the ASG to scale (as you don\u2019t need the processing power of the EC2 instances to serve the static content).\nRe-deploying the application in a VPC closer to the users may reduce latency (and therefore improve performance), but it doesn\u2019t solve the problem of reducing the need for the ASG to scale.\nUsing EFS instead of the EC2 root volume does not solve either problem.\nRedShift cannot be used to create content repositories to get content closer to users, it\u2019s a data warehouse used for analytics.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-cloudfront/\nhttps://aws.amazon.com/caching/cdn/"
    },
    {
        "question": "33. Question\nYou are a Solutions Architect at Digital Cloud Training. One of your clients has requested that you design a solution for distributing load across a number of EC2 instances across multiple AZs within a region. Customers will connect to several different applications running on the client\u2019s servers through their browser using multiple domain names and SSL certificates. The certificates are stored in AWS Certificate Manager (ACM).\nWhat is the optimal architecture to ensure high availability, cost effectiveness, and performance?\nLaunch a single ALB, configure host-based routing for the domain names and bind an SSL certificate to each routing rule\nLaunch a single ALB and bind multiple SSL certificates to the same secure listener. Clients will use the Server Name Indication (SNI) extension\nLaunch a single ALB and bind multiple SSL certificates to multiple secure listeners\nLaunch multiple ALBs and bind separate SSL certificates to each ELB\n",
        "answer": [
            2
        ],
        "explanation": "You can use a single ALB and bind multiple SSL certificates to the same listener\nWith Server Name Indication (SNI) a client indicates the hostname to connect to. SNI supports multiple secure websites using a single secure listener\nYou cannot have the same port in multiple listeners so adding multiple listeners would not work. Also, when using standard HTTP/HTTPS the port will always be 80/443 so you must be able to receive traffic on the same ports for multiple applications and still be able to forward to the correct instances. This is where host-based routing comes in\nWith host-based routing you can route client requests based on the Host field (domain name) of the HTTP header allowing you to route to multiple domains from the same load balancer (and share the same listener)\nYou do not need multiple ALBs and it would not be cost-effective\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/"
    },
    {
        "question": "34. Question\nThe website for a new application received around 50,000 requests each second and the company wants to use multiple applications to analyze the navigation patterns of the users on their website so they can personalize the user experience.\nWhat can a Solutions Architect use to collect page clicks for the website and process them sequentially for each user?\nAmazon SQS standard queue\nAWS CloudTrail trail\nAmazon Kinesis Streams\nAmazon SQS FIFO queue\n",
        "answer": [
            3
        ],
        "explanation": "This is a good use case for Amazon Kinesis streams as it is able to scale to the required load, allow multiple applications to access the records and process them sequentially\nAmazon Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to multiple Amazon Kinesis Applications\nAmazon Kinesis streams allows up to 1 MiB of data per second or 1,000 records per second for writes per shard. There is no limit on the number of shards so you can easily scale Kinesis Streams to accept 50,000 per second\nThe Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same record processor, making it easier to build multiple applications reading from the same Amazon Kinesis data stream\nStandard SQS queues do not ensure that messages are processed sequentially and FIFO SQS queues do not scale to the required number of transactions a second\nCloudTrail is used for auditing and is not useful here\nReferences:\nhttps://docs.aws.amazon.com/streams/latest/dev/service-sizes-and-limits.html\nhttps://aws.amazon.com/kinesis/data-streams/faqs/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-kinesis/"
    },
    {
        "question": "35. Question\nA retail organization is deploying a new application that will read and write data to a database. The company wants to deploy the application in three different AWS Regions in an active-active configuration. The databases need to replicate to keep information in sync.\nWhich solution best meets these requirements?\nAmazon DynamoDB with global tables\nAmazon Athena with Amazon S3 cross-region replication\nAWS Database Migration Service with change data capture\nAmazon Aurora Global Database\n",
        "answer": [
            1
        ],
        "explanation": "Amazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database. This is the only solution presented that provides an active-active configuration where reads and writes can take place in multiple regions with full bi-directional synchronization.\nAmazon Athena with S3 cross-region replication is not suitable. This is not a solution that provides a transactional database solution (Athena is used for analytics), or active-active synchronization.\nAmazon Aurora Global Database provides read access to a database in multiple regions \u2013 it does not provide active-active configuration with bi-directional synchronization (though you can failover to your read-only DBs and promote them to writable).\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/\nhttps://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/"
    },
    {
        "question": "36. Question\nYou are building an application that will collect information about user behavior. The application will rapidly ingest large amounts of dynamic data and requires very low latency. The database must be scalable without incurring downtime. Which database would you recommend for this scenario?\nRedShift\nDynamoDB\nRDS with MySQL\nRDS with Microsoft SQL\n",
        "answer": [
            2
        ],
        "explanation": "Amazon Dynamo DB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability\nPush button scaling means that you can scale the DB at any time without incurring downtime\nDynamoDB provides low read and write latency\nRDS uses EC2 instances so you have to change your instance type/size in order to scale compute vertically\nRedShift uses EC2 instances as well, so you need to choose your instance type/size for scaling compute vertically, but you can also scale horizontally by adding more nodes to the cluster\nRapid ingestion of dynamic data is not an ideal use case for RDS or RedShift\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/"
    },
    {
        "question": "37. Question\nYou have been asked to implement a solution for capturing, transforming and loading streaming data into an Amazon RedShift cluster. The solution will capture data from Amazon Kinesis Data Streams. Which AWS services would you utilize in this scenario? (choose 2)\nKinesis Data Firehose for capturing the data and loading it into RedShift\nKinesis Video Streams for capturing the data and loading it into RedShift\nLambda for transforming the data\nEMR for transforming the data\nAWS Data Pipeline for transforming the data\n",
        "answer": [
            1,
            3
        ],
        "explanation": "For this solution Kinesis Data Firehose can be used as it can use Kinesis Data Streams as a source and can capture, transform, and load streaming data into a RedShift cluster. Kinesis Data Firehose can invoke a Lambda function to transform data before delivering it to destinations\nKinesis Video Streams makes it easy to securely stream video from connected devices to AWS for analytics, machine learning (ML), and other processing, this solution does not involve video streams\nAWS Data Pipeline is used for processing and moving data between compute and storage services. It does not work with streaming data as Kinesis does\nElastic Map Reduce (EMR) is used for processing and analyzing data using the Hadoop framework. It is not used for transforming streaming data\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-kinesis/"
    },
    {
        "question": "38. Question\nA company is deploying a big data and analytics workload. The analytics will be run from a fleet of thousands of EC2 instances across multiple AZs. Data needs to be stored on a shared storage layer that can be mounted and accessed concurrently by all EC2 instances. Latency is not a concern however extremely high throughput is required.\nWhat storage layer would be most suitable for this requirement?\nAmazon EFS in Max I/O mode\nAmazon EFS in General Purpose mode\nAmazon EBS PIOPS\nAmazon S3\n",
        "answer": [
            1
        ],
        "explanation": "Amazon EFS file systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file operations\nAmazon S3 is not a storage layer that can be mounted and accessed concurrently\nAmazon EBS volumes cannot be shared between instances\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-efs/\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html"
    },
    {
        "question": "39. Question\nYour company is reviewing their information security processes. One of the items that came out of a recent audit is that there is insufficient data recorded about requests made to a few S3 buckets. The security team requires an audit trail for operations on the S3 buckets that includes the requester, bucket name, request time, request action, and response status.\nWhich action would you take to enable this logging?\nCreate a CloudWatch metric that monitors the S3 bucket operations and triggers an alarm\nEnable server access logging for the S3 buckets to save access logs to a specified destination bucket\nCreate a CloudTrail trail that audits S3 bucket operations\nEnable S3 event notifications for the specific actions and setup an SNS notification\n",
        "answer": [
            2
        ],
        "explanation": "Server access logging provides detailed records for the requests that are made to a bucket. To track requests for access to your bucket, you can enable server access logging. Each access log record provides details about a single access request, such as the requester, bucket name, request time, request action, response status, and an error code, if relevant\nFor capturing IAM/user identity information in logs you would need to configure AWS CloudTrail Data Events (however this does not audit the bucket operations required in the question)\nAmazon S3 event notifications can be sent in response to actions in Amazon S3 like PUTs, POSTs, COPYs, or DELETEs.S3 event notifications records the request action but not the other requirements of the security team\nCloudWatch metrics do not include the bucket operations specified in the question\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html"
    },
    {
        "question": "40. Question\nA Solutions Architect is designing a new architecture that will use an Amazon EC2 Auto Scaling group.\nWhich of the following factors determine the health check grace period? (choose 2)\nHow long the bootstrap script takes to run\nHow long it takes for the Auto Scaling group to detect a failure\nHow much of the application code is embedded in the AMI\nHow many Amazon CloudWatch alarms are configured for status checks\nHow frequently the Auto Scaling group scales up or down\n",
        "answer": [
            1,
            3
        ],
        "explanation": "Amazon EC2 Auto Scaling waits until the health check grace period ends before checking the health status of the instance. The length of the health check grace period needs to consider the warm-up time for your instances. This includes the time to start the application. Application code in the AMI as well as bootstrap scripts could delay application start-up, so you\u2019d want to consider these factors when determining the health check grace period.\nHow many times the Auto Scaling group scales up or down is not relevant to the health check grace period, every instance will need to go through this when launched and you need to ensure the instances start before the period ends.\nIt\u2019s not relevant how many CloudWatch alarms are configured for status checks as status checks are not acted on until the health check grace period ends.\nDetecting a failure is related to how quickly Auto Scaling can react and terminate and replace an instance, it\u2019s not relevant to the health check grace period.\nReferences:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/healthcheck.html\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/"
    },
    {
        "question": "41. Question\nA call center application consists of a three-tier application using Auto Scaling groups to automatically scale resources as needed. Users report that every morning at 9:00am the system becomes very slow for about 15 minutes. A Solutions Architect determines that a large percentage of the call center staff starts work at 9:00am, so Auto Scaling does not have enough time to scale to meet demand.\nHow can the Architect fix the problem?\nPermanently keep a steady state of instance that is needed at 9:00am to guarantee available resources, but use Spot Instances\nUse Reserved Instances to ensure the system has reserved the right amount of capacity for the scaling events\nCreate an Auto Scaling scheduled action to scale out the necessary resources at 8:30am each morning\nChange the Auto Scaling group\u2019s scale out event to scale based on network utilization\n",
        "answer": [
            3
        ],
        "explanation": "Scheduled scaling: Scaling based on a schedule allows you to set your own scaling schedule for predictable load changes. To configure your Auto Scaling group to scale based on a schedule, you create a scheduled action. This is ideal for situations where you know when and for how long you are going to need the additional capacity\nChanging the scale-out events to scale based on network utilization may not assist here. We\u2019re not certain the network utilization will increase sufficiently to trigger an Auto Scaling scale out action as the load may be more CPU/memory or number of connections. The main problem however is that we need to ensure the EC2 instances are provisioned ahead of demand not in response to demand (which would incur a delay whilst the EC2 instances \u201cwarm up\u201d)\nUsing reserved instances ensures capacity is available within an AZ, however the issue here is not that the AZ does not have capacity for more instances, it is that the instances are not being launched by Auto Scaling ahead of the peak demand\nKeeping a steady state of Spot instances is not a good solution. Spot instances may be cheaper, but this is not guaranteed and keeping them online 24hrs a day is wasteful and could prove more expensive\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html"
    },
    {
        "question": "42. Question\nYour company keeps unstructured data on a filesystem. You need to provide access to employees via EC2 instances in your VPC. Which storage solution should you choose?\nAmazon EBS\nAmazon Snowball\nAmazon EFS\nAmazon S3\n",
        "answer": [
            3
        ],
        "explanation": "EFS is the only storage system presented that provides a file system. EFS is accessed by mounting filesystems using the NFS v4.1 protocol from your EC2 instances. You can concurrently connect up to thousands of instances to a single EFS filesystem\nAmazon S3 is an object-based storage system that is accessed over a REST API\nAmazon EBS is a block-based storage system that provides volumes that are mounted to EC2 instances but cannot be shared between EC2 instances\nAmazon Snowball is a device used for migrating very large amounts of data into or out of AWS\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-efs/"
    },
    {
        "question": "43. Question\nYou are a Solutions Architect at a media company and you need to build an application stack that can receive customer comments from sporting events. The application is expected to receive significant load that could scale to millions of messages within a short space of time following high-profile matches. As you are unsure of the load required for the database layer what is the most cost-effective way to ensure that the messages are not dropped?\nUse RDS Auto Scaling for the database layer which will automatically scale as required\nCreate an SQS queue and modify the application to write to the SQS queue. Launch another application instance the polls the queue and writes messages to the database\nUse DynamoDB and provision enough write capacity to handle the highest expected load\nWrite the data to an S3 bucket, configure RDS to poll the bucket for new messages\n",
        "answer": [
            2
        ],
        "explanation": "Amazon Simple Queue Service (Amazon SQS) is a web service that gives you access to message queues that store messages waiting to be processed. SQS offers a reliable, highly-scalable, hosted queue for storing messages in transit between computers and is used for distributed/decoupled applications.\nThis is a great use case for SQS as the messages you don\u2019t have to over-provision the database layer or worry about messages being dropped\nRDS Auto Scaling does not exist. With RDS you have to select the underlying EC2 instance type to use and pay for that regardless of the actual load on the DB. Note that a new feature released in June 2019 does allow Auto Scaling for the RDS storage, but not the compute layer.\nWith DynamoDB there are now 2 pricing options:\nProvisioned capacity has been around forever and is one of the incorrect answers to this question. With provisioned capacity you have to specify the number of read/write capacity units to provision and pay for these regardless of the load on the database.\nWith the the new On-demand capacity mode DynamoDB is charged based on the data reads and writes your application performs on your tables. You do not need to specify how much read and write throughput you expect your application to perform because DynamoDB instantly accommodates your workloads as they ramp up or down. it might be a good solution to this question but is not an available option.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-sqs/"
    },
    {
        "question": "44. Question\nA company needs to store data for 5 years. The company will need to have immediate and highly available access to the data at any point in time but will not require frequent access.\nWhich lifecycle action should be taken to meet the requirements while reducing costs?\nTransition objects from Amazon S3 Standard to the GLACIER storage class\nTransition objects from Amazon S3 Standard to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)\nTransition objects to expire after 5 years\nTransition objects from Amazon S3 Standard to Amazon S3 Standard-Infrequent Access (S3 Standard-IA)\n",
        "answer": [
            4
        ],
        "explanation": "This is a good use case for S3 Standard-IA which provides immediate access and 99.9% availability.\nExpiring the objects after 5 years is going to delete them at the end of the 5-year period, but you still need to work out the best storage solution to use before then, and this answer does not provide a solution.\nThe S3 One Zone-IA tier provides immediate access, but the availability is lower at 99.5% so this is not the best option.\nThe Glacier storage class does not provide immediate access. You can retrieve within hours or minutes, but you do need to submit a job to retrieve the data.\nReferences:\nhttps://aws.amazon.com/s3/storage-classes/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "45. Question\nA Solutions Architect is designing an application that will run on Amazon ECS behind an Application Load Balancer (ALB). For security reasons, the Amazon EC2 host instances for the ECS cluster are in a private subnet.\nWhat should be done to ensure that the incoming traffic to the host instances is from the ALB only?\nUpdate the EC2 cluster security group to allow incoming access from the IP address of the ALB only\nModify the security group used by the EC2 cluster to allow incoming traffic from the security group used by the ALB only\nCreate network ACL rules for the private subnet to allow incoming traffic on ports 32768 through 61000 from the IP address of the ALB only\nEnable AWS WAF on the ALB and enable the ECS rule\n",
        "answer": [
            2
        ],
        "explanation": "The best way to accomplish this requirement is to restrict incoming traffic to the Security Group used by the ALB. This will ensure that only the ALB (and its nodes) will be able to connect to the EC2 instances in the ECS cluster.\nYou should not use the IP address of the ALB in the Security Group rules as an ALB has multiple nodes in each AZ in which it has subnets defined. Always use security groups whenever you can.\nNetwork ACLs work at the subnet level. It is preferable to use Security Groups which work at the instance level. Also, you should not use the IP of the ALB as it will have multiple nodes / IPs and it would be cumbersome to setup and administer.\nEnabling a WAF is useful when you need to protect against malicious code. However, this is not a requirement for this solution, you just need to restrict incoming traffic to the ALB.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/"
    },
    {
        "question": "46. Question\nA Solutions Architect is designing a highly-scalable system to track records. Records must remain available for immediate download for three months, and then the records must be deleted.\nWhat\u2019s the most appropriate decision for this use case?\nStore the files on Amazon EFS, and create a lifecycle policy to remove the files after three months\nStore the files on Amazon S3, and create a lifecycle policy to remove the files after three months\nStore the files on Amazon EBS, and create a lifecycle policy to remove the files after three months\nStore the files on Amazon Glacier, and create a lifecycle policy to remove the files after three months\n",
        "answer": [
            2
        ],
        "explanation": "With S3 you can create a lifecycle action using the \u201cexpiration action element\u201d which expires objects (deletes them) at the specified time\nS3 lifecycle actions apply to any storage class, including Glacier, however Glacier would not allow immediate download\nThere is no lifecycle policy available for deleting files on EBS and EFS\nNOTE: The new Amazon Data Lifecycle Manager (DLM) feature automates the creation, retention, and deletion of EBS snapshots but not the individual files within an EBS volume. This is a new feature that may not yet feature on the exam\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "47. Question\nA Solutions Architect needs to allow another AWS account programmatic access to upload objects to his bucket. The Solutions Architect needs to ensure that he retains full control of the objects uploaded to the bucket. How can this be done?\nThe Architect will need to instruct the user in the other AWS account to grant him access when uploading objects\nThe Architect will need to take ownership of objects after they have been uploaded\nThe Architect can use a resource-based bucket policy that grants cross-account access and include a conditional statement that only allows uploads if full control access is granted to the Architect\nThe Architect can use a resource-based ACL with an IAM policy that grants cross-account access and include a conditional statement that only allows uploads if full control access is granted to the Architect\n",
        "answer": [
            3
        ],
        "explanation": "You can use a resource-based bucket policy to allow another AWS account to upload objects to your bucket and use a conditional statement to ensure that full control permissions are granted to a specific account identified by an ID (e.g. email address)\nYou cannot use a resource-based ACL with IAM policy as this configuration does not support conditional statements\nTaking ownership of objects is not a concept that is valid in Amazon S3 and asking the user in the other AWS account to grant access when uploading is not a good method as technical controls to enforce this behavior are preferred\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "48. Question\nYou are creating a design for a web-based application that will be based on a web front-end using EC2 instances and a database back-end. This application is a low priority and you do not want to incur costs in general day to day management. Which AWS database service can you use that will require the least operational overhead?\nDynamoDB\nEMR\nRedShift\nRDS\n",
        "answer": [
            1
        ],
        "explanation": "Out of the options in the list, DynamoDB requires the least operational overhead as there are no backups, maintenance periods, software updates etc. to deal with\nRDS, RedShift and EMR all require some operational overhead to deal with backups, software updates and maintenance periods\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/"
    },
    {
        "question": "49. Question\nA company\u2019s Amazon RDS MySQL DB instance may be rebooted for maintenance and to apply patches. This database is critical and potential user disruption must be minimized.\nWhat should the Solution Architect do in this scenario?\nSet up an Amazon RDS MySQL cluster\nCreate an RDS MySQL Read Replica\nSet the Amazon RDS MySQL to Multi-AZ\nCreate an Amazon EC2 instance MySQL cluster\n",
        "answer": [
            3
        ],
        "explanation": "With RDS in multi-AZ configuration system upgrades like OS patching, DB Instance scaling and system upgrades, are applied first on the standby, before failing over and modifying the other DB Instance. This means the database is always available with minimal disruption.\nYou cannot create a \u201cRDS MySQL cluster\u201d with Amazon RDS. If you want to create a MySQL cluster you need to install on EC2 (which is another option presented). If you install in EC2 you must manage the whole process of patching and failover yourself as it\u2019s not a managed solution.\nAmazon RDS MySQL Read Replicas can serve reads but not writes so there would be a disruption if the application is writing to the DB while the system updates are taking place.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-rds/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html"
    },
    {
        "question": "50. Question\nYou would like to share some documents with public users accessing an S3 bucket over the Internet. What are two valid methods of granting public read permissions so you can share the documents? (choose 2)\nGrant public read access to the objects when uploading\nGrant public read on all objects using the S3 bucket ACL\nShare the documents using CloudFront and a static website\nUse the AWS Policy Generator to create a bucket policy for your Amazon S3 bucket granting read access to public anonymous users\nShare the documents using a bastion host in a public subnet\n",
        "answer": [
            1,
            4
        ],
        "explanation": "Access policies define access to resources and can be associated with resources (buckets and objects) and users\nYou can use the AWS Policy Generator to create a bucket policy for your Amazon S3 bucket. Bucket policies can be used to grant permissions to objects\nYou can define permissions on objects when uploading and at any time afterwards using the AWS Management Console.\nYou cannot use a bucket ACL to grant permissions to objects within the bucket. You must explicitly assign the permissions to each object through an ACL attached as a subresource to that object\nUsing an EC2 instance as a bastion host to share the documents is not a feasible or scalable solution\nYou can configure an S3 bucket as a static website and use CloudFront as a front-end however this is not necessary just to share the documents and imposes some constraints on the solution\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "51. Question\nYou have created a private Amazon CloudFront distribution that serves files from an Amazon S3 bucket and is accessed using signed URLs. You need to ensure that users cannot bypass the controls provided by Amazon CloudFront and access content directly.\nHow can this be achieved? (choose 2)\nCreate an origin access identity and associate it with your distribution\nModify the Edge Location to restrict direct access to Amazon S3 buckets\nModify the permissions on the origin access identity to restrict read access to the Amazon S3 bucket\nCreate a new signed URL that requires users to access the Amazon S3 bucket through Amazon CloudFront\nModify the permissions on the Amazon S3 bucket so that only the origin access identity has read and download permissions\n",
        "answer": [
            1,
            5
        ],
        "explanation": "If you\u2019re using an Amazon S3 bucket as the origin for a CloudFront distribution, you can either allow everyone to have access to the files there, or you can restrict access. If you limit access by using CloudFront signed URLs or signed cookies you also won\u2019t want people to be able to view files by simply using the direct URL for the file. Instead, you want them to only access the files by using the CloudFront URL, so your protections work. This can be achieved by creating an OAI and associating it with your distribution and then modifying the permissions on the S3 bucket to only allow the OAI to access the files\nYou do not modify permissions on the OAI \u2013 you do this on the S3 bucket\nIf users are accessing the S3 files directly, a new signed URL is not going to stop them\nYou cannot modify edge locations to restrict access to S3 buckets\nReferences:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html"
    },
    {
        "question": "52. Question\nYour company shares some HR videos stored in an Amazon S3 bucket via CloudFront. You need to restrict access to the private content so users coming from specific IP addresses can access the videos and ensure direct access via the Amazon S3 bucket is not possible.\nHow can this be achieved?\nConfigure CloudFront to require users to access the files using a signed URL, and configure the S3 bucket as a website endpoint\nConfigure CloudFront to require users to access the files using signed cookies, and move the files to an encrypted EBS volume\nConfigure CloudFront to require users to access the files using signed cookies, create an origin access identity (OAI) and instruct users to login with the OAI\nConfigure CloudFront to require users to access the files using a signed URL, create an origin access identity (OAI) and restrict access to the files in the Amazon S3 bucket to the OAI\n",
        "answer": [
            4
        ],
        "explanation": "A signed URL includes additional information, for example, an expiration date and time, that gives you more control over access to your content. You can also specify the IP address or range of IP addresses of the users who can access your content\nIf you use CloudFront signed URLs (or signed cookies) to limit access to files in your Amazon S3 bucket, you may also want to prevent users from directly accessing your S3 files by using Amazon S3 URLs. To achieve this, you can create an origin access identity (OAI), which is a special CloudFront user, and associate the OAI with your distribution. You can then change the permissions either on your Amazon S3 bucket or on the files in your bucket so that only the origin access identity has read permission (or read and download permission)\nUsers cannot login with an OAI\nYou cannot use CloudFront and an OAI when you\u2019re S3 bucket is configured as a website endpoint\nYou cannot use CloudFront to pull data directly from an EBS volume\nReferences:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "53. Question\nThere is a temporary need to share some video files that are stored in a private S3 bucket. The consumers do not have AWS accounts and you need to ensure that only authorized consumers can access the files. What is the best way to enable this access?\nEnable public read access for the S3 bucket\nUse CloudFront to distribute the files using authorization hash tags\nGenerate a pre-signed URL and distribute it to the consumers\nConfigure an allow rule in the Security Group for the IP addresses of the consumers\n",
        "answer": [
            3
        ],
        "explanation": "S3 pre-signed URLs can be used to provide temporary access to a specific object to those who do not have AWS credentials. This is the best option\nEnabling public read access does not restrict the content to authorized consumers\nYou cannot use CloudFront as hash tags are not a CloudFront authentication mechanism\nSecurity Groups do not apply to S3 buckets\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/"
    },
    {
        "question": "54. Question\nYou would like to provide some on-demand and live streaming video to your customers. The plan is to provide the users with both the media player and the media files from the AWS cloud. One of the features you need is for the content of the media files to begin playing while the file is still being downloaded.\nWhat AWS services can deliver these requirements? (choose 2)\nStore the media files in an S3 bucket\nUse CloudFront with a Web and RTMP distribution\nStore the media files on an EBS volume\nUse CloudFront with an RTMP distribution\nStore the media files on an EC2 instance\n",
        "answer": [
            1,
            2
        ],
        "explanation": "For serving both the media player and media files you need two types of distributions:\n\u2013         A web distribution for the media player\n\u2013         An RTMP distribution for the media files\nRTMP:\n\u2013         Distribute streaming media files using Adobe Flash Media Server\u2019s RTMP protocol\n\u2013         Allows an end user to begin playing a media file before the file has finished downloading from a CloudFront edge location\n\u2013         Files must be stored in an S3 bucket (not an EBS volume or EC2 instance)\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-cloudfront/"
    },
    {
        "question": "55. Question\nThe company you work for is currently transitioning their infrastructure and applications into the AWS cloud. You are planning to deploy an Elastic Load Balancer (ELB) that distributes traffic for a web application running on EC2 instances. You still have some application servers running on-premise and you would like to distribute application traffic across both your AWS and on-premises resources.\nHow can this be achieved?\nProvision a Direct Connect connection between your on-premises location and AWS and create a target group on an ALB to use IP based targets for both your EC2 instances and on-premises servers\nProvision a Direct Connect connection between your on-premises location and AWS and create a target group on an ALB to use Instance ID based targets for both your EC2 instances and on-premises servers\nProvision an IPSec VPN connection between your on-premises location and AWS and create a CLB that uses cross-zone load balancing to distributed traffic across EC2 instances and on-premises servers\nThis cannot be done, ELBs are an AWS service and can only distributed traffic within the AWS cloud\n",
        "answer": [
            1
        ],
        "explanation": "The ALB (and NLB) supports IP addresses as targets\nUsing IP addresses as targets allows load balancing any application hosted in AWS or on-premises using IP addresses of the application back-ends as targets\nYou must have a VPN or Direct Connect connection to enable this configuration to work\nYou cannot use instance ID based targets for on-premises servers and you cannot mix instance ID and IP address target types in a single target group\nThe CLB does not support IP addresses as targets\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/\nhttps://aws.amazon.com/blogs/aws/new-application-load-balancing-via-ip-address-to-aws-on-premises-resources/"
    },
    {
        "question": "56. Question\nA Solutions Architect is developing a mobile web app that will provide access to health-related data. The web apps will be tested on Android and iOS devices. The Architect needs to run tests on multiple devices simultaneously and to be able to reproduce issues, and record logs and performance data to ensure quality before release.\nWhat AWS service can be used for these requirements?\nAWS Cognito\nAWS Device Farm\nAWS Workspaces\nAmazon Appstream 2.0\n",
        "answer": [
            2
        ],
        "explanation": "AWS Device Farm is an app testing service that lets you test and interact with your Android, iOS, and web apps on many devices at once, or reproduce issues on a device in real time\nAmazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. It is not used for testing\nAmazon WorkSpaces is a managed, secure cloud desktop service\nAmazon AppStream 2.0 is a fully managed application streaming service\nReferences:\nhttps://aws.amazon.com/device-farm/"
    },
    {
        "question": "57. Question\nThere is a new requirement to implement in-memory caching for a Financial Services application due to increasing read-heavy load. The data must be stored persistently. Automatic failover across AZs is also required.\nWhich two items from the list below are required to deliver these requirements? (choose 2)\nElastiCache with the Memcached engine\nMulti-AZ with Cluster mode and Automatic Failover enabled\nElastiCache with the Redis engine\nMultiple nodes placed in different AZs\nRead replica with failover mode enabled\n",
        "answer": [
            2,
            3
        ],
        "explanation": "Redis engine stores data persistently\nMemached engine does not store data persistently\nRedis engine supports Multi-AZ using read replicas in another AZ in the same region\nYou can have a fully automated, fault tolerant ElastiCache-Redis implementation by enabling both cluster mode and multi-AZ failover\nMemcached engine does not support Multi-AZ failover or replication\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-elasticache/"
    },
    {
        "question": "58. Question\nAn application you are designing receives and processes files. The files are typically around 4GB in size and the application extracts metadata from the files which typically takes a few seconds for each file. The pattern of updates is highly dynamic with times of little activity and then multiple uploads within a short period of time.\nWhat architecture will address this workload the most cost efficiently?\nUpload files into an S3 bucket, and use the Amazon S3 event notification to invoke a Lambda function to extract the metadata\nStore the file in an EBS volume which can then be accessed by another EC2 instance for processing\nPlace the files in an SQS queue, and use a fleet of EC2 instances to extract the metadata\nUse a Kinesis data stream to store the file, and use Lambda for processing\n",
        "answer": [
            1
        ],
        "explanation": "Storing the file in an S3 bucket is the most cost-efficient solution, and using S3 event notifications to invoke a Lambda function works well for this unpredictable workload\nKinesis data streams runs on EC2 instances and you must therefore provision some capacity even when the application is not receiving files. This is not as cost-efficient as storing them in an S3 bucket prior to using Lambda for the processing\nSQS queues have a maximum message size of 256KB. You can use the extended client library for Java to use pointers to a payload on S3 but the maximum payload size is 2GB\nStoring the file in an EBS volume and using EC2 instances for processing is not cost efficient\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html"
    },
    {
        "question": "59. Question\nA Solutions Architect needs to transform data that is being uploaded into S3. The uploads happen sporadically and the transformation should be triggered by an event. The transformed data should then be loaded into a target data store.\nWhat services would be used to deliver this solution in the MOST cost-effective manner? (choose 2)\nUse AWS Glue to extract, transform and load the data into the target data store\nConfigure CloudFormation to provision a Kinesis data stream to transform the data and load it into S3\nConfigure S3 event notifications to trigger a Lambda function when data is uploaded and use the Lambda function to trigger the ETL job\nConfigure CloudFormation to provision AWS Data Pipeline to transform the data\nConfigure a CloudWatch alarm to send a notification to CloudFormation when data is uploaded\n",
        "answer": [
            1,
            3
        ],
        "explanation": "S3 event notifications triggering a Lambda function is completely serverless and cost-effective\nAWS Glue can trigger ETL jobs that will transform that data and load it into a data store such as S3\nKinesis Data Streams is used for processing data, rather than extracting and transforming it. The Kinesis consumers are EC2 instances which are not as cost-effective as serverless solutions\nAWS Data Pipeline can be used to automate the movement and transformation of data, it relies on other services to actually transform the data\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\nhttps://aws.amazon.com/glue/"
    },
    {
        "question": "60. Question\nA company has divested a single business unit and needs to move the AWS account owned by the business unit to another AWS Organization. How can this be achieved?\nMigrate the account using the AWS Organizations console\nCreate a new account in the destination AWS Organization and migrate resources\nCreate a new account in the destination AWS Organization and share the original resources using AWS Resource Access Manager\nMigrate the account using AWS CloudFormation\n",
        "answer": [
            1
        ],
        "explanation": "Accounts can be migrated between organizations. To do this you must have root or IAM access to both the member and master accounts. Resources will remain under the control of the migrated account.\nYou do not need to use AWS CloudFormation. You can use the Organizations API or AWS CLI for when there are many accounts to migrate and therefore you could use CloudFormation for any additional automation but it is not necessary for this scenario.\nYou do not need to create a new account in the destination AWS Organization as you can just migrate the existing account.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-organizations/\nhttps://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/"
    },
    {
        "question": "61. Question\nA new application is to be published in multiple regions around the world. The Architect needs to ensure only 2 IP addresses need to be whitelisted. The solution should intelligently route traffic for lowest latency and provide fast regional failover.\nHow can this be achieved?\nLaunch EC2 instances into multiple regions behind an ALB and use Amazon CloudFront with a pair of static IP addresses\nLaunch EC2 instances into multiple regions behind an NLB and use AWS Global Accelerator\nLaunch EC2 instances into multiple regions behind an ALB and use a Route 53 failover routing policy\nLaunch EC2 instances into multiple regions behind an NLB with a static IP address\n",
        "answer": [
            2
        ],
        "explanation": "AWS Global Accelerator uses the vast, congestion-free AWS global network to route TCP and UDP traffic to a healthy application endpoint in the closest AWS Region to the user. This means it will intelligently route traffic to the closest point of presence (reducing latency). Seamless failover is ensured as AWS Global Accelerator uses anycast IP address which means the IP does not change when failing over between regions so there are no issues with client caches having incorrect entries that need to expire. This is the only solution that provides deterministic failover.\nAn NLB with a static IP is a workable solution as you could configure a primary and secondary address in applications. However, this solution does not intelligently route traffic for lowest latency.\nA Route 53 failover routing policy uses a primary and standby configuration. Therefore, it sends all traffic to the primary until it fails a health check at which time it sends traffic to the secondary. This solution does not intelligently route traffic for lowest latency.\nAmazon CloudFront cannot be configured with \u201ca pair of static IP addresses\u201d.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/aws-global-accelerator/\nhttps://aws.amazon.com/global-accelerator/\nhttps://aws.amazon.com/global-accelerator/faqs/\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html"
    },
    {
        "question": "62. Question\nAn e-commerce web application needs a highly scalable key-value database. Which AWS database service should be used?\nAmazon DynamoDB\nAmazon ElastiCache\nAmazon RedShift\nAmazon RDS\n",
        "answer": [
            1
        ],
        "explanation": "A key-value database is a type of nonrelational (NoSQL) database that uses a simple key-value method to store data. A key-value database stores data as a collection of key-value pairs in which a key serves as a unique identifier. Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability \u2013 this is the best database for these requirements.\nAmazon RDS is a relational (SQL) type of database, not a key-value / nonrelational database.\nAmazon RedShift is a data warehouse service used for online analytics processing (OLAP) workloads.\nAmazon ElastiCache is an in-memory caching database. This is not a nonrelational key-value database.\nReferences:\nhttps://aws.amazon.com/nosql/key-value/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/"
    },
    {
        "question": "63. Question\nAn Architect needs to find a way to automatically and repeatably create many member accounts within an AWS Organization. The accounts also need to be moved into an OU and have VPCs and subnets created.\nWhat is the best way to achieve this?\nUse the AWS Management Console\nUse CloudFormation with scripts\nUse the AWS Organizations API\nUse the AWS CLI\n",
        "answer": [
            2
        ],
        "explanation": "The best solution is to use a combination of scripts and AWS CloudFormation. You will also leverage the AWS Organizations API. This solution can provide all of the requirements.\nYou can create member accounts with the AWS Organizations API. However, you cannot use that API to configure the account and create VPCs and subnets.\nUsing the AWS Management Console is not a method of automatically creating the resources.\nYou can do all tasks using the AWS CLI but it is better to automate the process using AWS CloudFormation.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-organizations/\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-organizations-to-automate-end-to-end-account-creation/"
    },
    {
        "question": "64. Question\nA company has acquired another business and needs to migrate their 50TB of data into AWS within 1 month. They also require a secure, reliable and private connection to the AWS cloud.\nHow are these requirements best accomplished?\nProvision an AWS Direct Connect connection and migrate the data over the link\nLaunch a Virtual Private Gateway (VPG) and migrate the data over the AWS VPN\nMigrate data using AWS Snowball. Provision an AWS VPN initially and order a Direct Connect link\nProvision an AWS VPN CloudHub connection and migrate the data over redundant links\n",
        "answer": [
            3
        ],
        "explanation": "AWS Direct Connect provides a secure, reliable and private connection. However, lead times are often longer than 1 month so it cannot be used to migrate data within the timeframes. Therefore, it is better to use AWS Snowball to move the data and order a Direct Connect connection to satisfy the other requirement later on. In the meantime, the organization can use an AWS VPN for secure, private access to their VPC.\nA VPG is the AWS-side of an AWS VPN. A VPN does not provide a private connection and is not reliable as you can never guarantee the latency over the Internet.\nAWS VPN CloudHub is a service for connecting multiple sites into your VPC over VPN connections. It is not used for aggregating links and the limitations of Internet bandwidth from the company where the data is stored will still be an issue. It also uses the public Internet so is not a private or reliable connection.\nReferences:\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/aws-direct-connect/\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/migration/aws-snowball/\nhttps://aws.amazon.com/snowball/\nhttps://aws.amazon.com/directconnect/"
    },
    {
        "question": "65. Question\nAn organization in the health industry needs to create an application that will transmit protected health data to thousands of service consumers in different AWS accounts. The application servers run on EC2 instances in private VPC subnets. The routing for the application must be fault tolerant.\nWhat should be done to meet these requirements?\nCreate a proxy server in the service provider VPC to route requests from service consumers to the application servers\nCreate an internal Application Load Balancer in the service provider VPC and put application servers behind it\nCreate a virtual private gateway connection between each pair of service provider VPCs and service consumer VPCs\nCreate a VPC endpoint service and grant permissions to specific service consumers to create a connection\n",
        "answer": [
            4
        ],
        "explanation": "What you need to do here is offer the service through a service provider offering. This is a great use case for a VPC endpoint service using AWS PrivateLink (referred to as an endpoint service). Other AWS principals can then create a connection from their VPC to your endpoint service using an interface VPC endpoint. You are acting as the service provider and offering the service to service consumers. This configuration uses a Network Load Balancer and can be fault tolerant by configuring multiple subnets in which the EC2 instances are running.\nCreating a virtual private gateway connection between each pair of service provider VPCs and service consumer VPCs would be extremely cumbersome and is not the best option.\nCreating an internal ALB would not work as you need consumers from outside your VPC to be able to connect.\nUsing a proxy service is possible but would not scale as well and would present a single point of failure unless there is some load balancing to multiple proxies (not mentioned).\nReferences:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html\nhttps://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/\n\n*27\n*28\n*30\n*33\n*37\n*40\n*44\n*45\n*61"
    }
]